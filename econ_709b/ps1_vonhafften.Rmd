---
title: "ECON 709B - Problem Set 1"
author: "Alex von Hafften^[I worked on this problem set with a study group of Michael Nattinger, Andrew Smith, and Ryan Mather. I also discussed problems with Emily Case, Sarah Bass, and Danny Edgel.]"
date: "11/11/2020"
output: pdf_document
header-includes:
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \newcommand{\var}{\text{var}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. 2.1 - 2.2\footnote{These problems come from \textit{Econometrics} by Bruce Hansen, revised on October 23, 2020.}

2.1 Find $E[E[E[Y|X_1, X_2, X_3]|X_1, X_2]|X_1]$.

Using the law of iterated expectations,

$$
E[E[E[Y|X_1, X_2, X_3]|X_1, X_2]|X_1]
=E[E[Y|X_1, X_2]|X_1]
=E[Y|X_1]
$$

2.2 If $E[Y|X]=a+bX$, find $E[YX]$ as a function of moments of $X$.

Using the law of iterated expectations,

$$
E[YX] = E[E[YX|X]] = E[XE[Y|X]] = E[X(a+bX)]=E[aX+bX^2]=aE[X]+bE[X^2]
$$

2. 2.3 Prove conclusion (4) of Theorem 2.4.

If $E|Y| < \infty$ then for any function $h(x)$ such that $E|h(X)e| < \infty$ then $E[h(X)e] = 0$.

Proof: Using the law of iterated expectations, Theorem 2.3, and conclusion (1) (i.e., $E[e | X] = 0$),

$$
E[h(X)e] = E[E[h(X)e|X]] = E[h(X)E[e|X]]= E[h(X)(0)]=E[0]=0
$$

$\square$

\pagebreak

3. 2.4 Suppose that the random variables $Y$ and $X$ only take the values 0 and 1, and have the following joint probability distribution

+------+------+------+
|      |$X=0$ |$X=1$ |
+------+------+------+
|$Y=0$ |.1    | .2   |
+------+------+------+
|$Y=1$ |.4    | .3   |
+------+------+------+

Find $E[Y|X]$, $E[Y^2|X]$ and $\var[Y|X]$ for $X=0$, $X=1$.

\begin{align*}
E[Y|X=0] &= (1)P[Y=1|X=0] + (0)P[Y=0|X=0] = (1)(.4)/(.5)=.8 \\
E[Y|X=1] &= (1)P[Y=1|X=1] + (0)P[Y=0|X=1] = (1)(.3)/(.5)=.6 \\
E[Y^2|X=0] &= (1)^2P[Y=1|X=0] + (0)^2P[Y=0|X=0] = (1)^2(.4)/(.5)=.8 \\
E[Y^2|X=1] &= (1)^2P[Y=1|X=1] + (0)^2P[Y=0|X=1] = (1)^2(.3)/(.5)=.6 \\
\var[Y|X=0] &= E[Y^2|X=0] - (E[Y|X=0])^2 = (.8)-(.8)^2 = 0.16 \\
\var[Y|X=1] &= E[Y^2|X=1] - (E[Y|X=1])^2 = (.6)-(.6)^2 = 0.24
\end{align*}

4. 2.5 (c) Show that $\sigma^2(X)$ is the best predictor of $e^2$ given $X$. Show that $\sigma^2(X)$ minimizes the mean-squared error and is thus the best predictor.

........



5. 2.8 Suppose that $Y$ is discrete-valued, taking values only on the non-negative integers, and the conditional distribution of $Y$ given $X = x$ is Poisson: $P[Y=j|X=x] = \frac{\exp(-x'\beta)(x'\beta)^j}{j!}, j=0, 1, 2,...$. Compute $E[Y|X]$ and $\var[Y|X]$. Does this justify a linear regression model of the form $Y=X'\beta + e$?\footnote{Hint: $P[Y=j] = \frac{\exp(-\lambda)(\lambda)^j}{j!}$, then $E[Y] = \lambda$ and $\var[Y] = \lambda$.}

Using the hint, we know that $E[Y|X] = x'\beta$ and $\var[Y|X] = x'\beta$.

Yes, this justifies a linear regression model because $E[e|X] = E[Y - X'\beta | X] = E[Y|X] - E[X'\beta | X] = x'\beta - x'\beta = 0$.

6. 2.10 - 2.14 Explain your answers.

2.10 If $Y=X\beta + e, X \in \R$, and $E[e|X] = 0$, then $E[X^2e] = 0$.

True, based on the law of iterated expectation:

$$
E[X^2e] = E[E[X^2e|X]] = E[X^2E[e|X]]= E[X^2(0)]=E[0]=0
$$

2.11 If $Y=X\beta + e, X \in \R$, and $E[Xe] = 0$, then $E[X^2e] = 0$.

False, for a counter example, assume $X \sim N(0,1)$ and $e$ is a degenerate random variable equal to 1. Notice that $E[Xe]=E[X] =0$ and $E[X^2e]=E[X^2]=1$.

2.12 If $Y=X'\beta + e$, and $E[e|X] = 0$, then $e$ is independent of $X$.

False, for a counter example, assume $X = (X_1, ..., X_k)'$ and $U= (U_1, ..., U_k)'$ where $X_1,...,X_k$ and $U_1, ..., U_k$ are independently distributed standard normal.  Let $e = X'U$.  Thus, conditional on $X$, $e$ is distributed $N(0, X'X)$, so $E[e|X]=0$, but $X$ and $e$ are not independent.

2.13 If $Y=X'\beta + e$, and $E[Xe] = 0$, then $E[e|X]=0$.

False, for a counter example, assume $X = (X_1, ..., X_k)'$ where $X_1,...,X_k \sim N(0,1)$ and $e$ is a degenerate random variable equal to 1. Notice that $E[Xe]=E[X] =0$ and $E[e|X]=E[e]=1$.

2.14 If $Y=X'\beta + e$, and $E[e|X] = 0$, and $E[e^2|X]=\sigma^2$, then $e$ is independent of $X$.

False, ...

\pagebreak

7. 2.16 Let $X$ and $Y$ have the joint density $f(x, y) = \frac{3}{2} (x^2 + y^2)$ on $0 \le x \le 1, 0 \le y \le 1$. Compute the coefficients of the best linear predictor $Y = \alpha + \beta X + e$. Compute the conditional expectation $m(x) = E[Y|X=x]$. Are the best linear predictor and conditional expectation different?

Best Linear Predictor (BLP):

$$
f_X(x) 
= \int_{-\infty}^\infty f(x,y) dy 
= \int_0^1 \frac{3}{2} (x^2 + y^2) dy 
= \frac{3}{2} \Bigg[x^2y+\frac{y^3}{3} \Bigg]_{y=0}^1
= \frac{3}{2}x^2 + \frac{1}{2}
$$

$$
E[X] 
= \int_0^1 x \Bigg( \frac{3}{2}x^2 + \frac{1}{2} \Bigg) dx 
= \Bigg[\frac{3}{8}x^4 + \frac{1}{4}x^2 \Bigg]_0^1 
= \frac{5}{8}
$$

$$
E[X^2] 
= \int_0^1 x^2 \Bigg( \frac{3}{2}x^2 + \frac{1}{2} \Bigg) dx 
= \Bigg[\frac{3}{10}x^5 + \frac{1}{6}x^3 \Bigg]_0^1 
= \frac{7}{15}
$$

Since the joint distribution is symmetric, $E[Y] = \frac{5}{8}$ and $E[Y^2] = \frac{7}{15}$.

\begin{align*} 
E[XY]
&= \int_0^1\int_0^1 xy \frac{3}{2} (x^2 + y^2) dxdy\\
&= \frac{3}{2} \int_0^1\int_0^1 x^3y + xy^3 dxdy\\
&= \frac{3}{2} \int_0^1 \Bigg[ \frac{x^4y}{4} + \frac{x^2y^3}{2} \Bigg]_{x=0}^1 dy\\
&= \frac{3}{2} \int_0^1 \frac{y}{4} + \frac{y^3}{2} dy\\
&= \frac{3}{2} \Bigg[ \frac{y^2}{8} + \frac{y^4}{8} \Bigg]_0^1\\
&= \frac{3}{8}
\end{align*} 

\begin{align*} 
S(\alpha, \beta) 
&=E[(Y- \alpha - \beta X)^2] \\
&=  E[Y^2- \alpha Y - \beta XY - \alpha Y + \alpha^2 + \alpha\beta X - \beta XY+ \alpha\beta X  + \beta^2 X^2] \\
&=  E[Y^2] - \alpha E[Y] - \beta E[XY] - \alpha E[Y] + \alpha^2 + \alpha\beta E[X] - \beta E[XY] + \alpha\beta E[X]  + \beta^2 E[X^2] \\
&=  \frac{7}{15} - \frac{5}{8} \alpha  - \frac{3}{8} \beta  - \frac{5}{8} \alpha  + \alpha^2 + \frac{5}{8} \alpha\beta  - \frac{3}{8} \beta  + \frac{5}{8} \alpha \beta + \frac{7}{15} \beta^2  \\
&=  \frac{7}{15} - \frac{5}{4} \alpha - \frac{3}{4} \beta  + \alpha^2 + \frac{5}{4} \alpha \beta + \frac{7}{15} \beta^2  \\
\end{align*} 

FOC [$\alpha$]:

$$
0 = - \frac{5}{4} +2\alpha + \frac{5}{4} \beta \implies
\beta = 1- \frac{8}{5} \alpha
$$

\pagebreak

FOC [$\beta$]:

$$
0 = - \frac{3}{4} + \frac{5}{4} \alpha + \frac{14}{15} \beta \implies
45 = 75 \alpha + 56 \beta 
$$

$$
45 = 75 \alpha + 56 \Bigg(1- \frac{8}{5} \alpha\Bigg) 
\implies \alpha = \frac{55}{73} 
\implies \beta = \frac{-15}{73} 
$$

Conditional Expectation Function (CEF):

$$
f_{Y|X} (y|x) 
= \frac{f(x, y)}{f_X(x)} 
= \frac{\frac{3}{2} (x^2 + y^2)}{\frac{3}{2}x^2 + \frac{1}{2}}
= \frac{3 x^2 + 3y^2}{3x^2 + 1}
$$

$$
m(x) 
= E[Y|X=x] 
= \int_{-\infty}^\infty y f_{Y|X} (y|x) dy 
= \int_0^1 \frac{3 x^2y + 3y^3}{3x^2 + 1} dy 
= \Bigg[ \frac{\frac{3}{2} x^2y^2 + \frac{3}{4}y^4}{3x^2 + 1} \Bigg]_{y=0}^1
= \frac{6 x^2 + 3}{12x^2 + 4}
$$

```{r, echo = FALSE}
plot(1, type = "n", xlim=c(0,1), ylim = c(.5,.8), xlab = "x", ylab="y")

x= seq(0, 1, .01)

cef = (6 * x^2 + 3)/(12 * x^2 + 4)
blp = 55/73 -15/73 *x

lines(x=x, y=cef, col = "red")
lines(x=x, y=blp, col = "blue")

text("CEF", col = "red", x= .1, y=.75, pos =3)
text("BLP", col = "blue", x= .1, y=.7)
```

\pagebreak

8. 4.1 - 4.6

4.1 For some integer $k$, set $\mu_k = E[Y^k]$.

(a) Construct an estimator $\hat{\mu}_k$ for $\mu_k$.

$$
\hat{\mu}_k = \frac{1}{n} \sum_{i=0}^n Y_i^k
$$

(b) Show that $\hat{\mu}_k$ is unbiased for $\mu_k$.

$$
E[\hat{\mu}_k] = E\Bigg[\frac{1}{n} \sum_{i=0}^n Y_i^k \Bigg] = \frac{1}{n} \sum_{i=0}^n E[Y_i^k] = \frac{1}{n} \sum_{i=0}^n \mu_k = \mu_k
$$

(c) Calculate the variance of $\hat{\mu}_k$, say $\var[\hat{\mu}_k]$. What assumption is needed for $\var[\hat{\mu}_k]$ to be finite?

We need to assume that $\mu_{2k} < \infty$ for $\var[\hat{\mu}_k]$ to be finite:

$$
\var[\hat{\mu}_k] 
= \var \Bigg[ \frac{1}{n} \sum_{i=0}^n Y_i^k \Bigg] 
= \frac{1}{n^2} \sum_{i=0}^n \var [Y_i^k ] 
= \frac{1}{n^2} \sum_{i=0}^n (E[Y_i^{2k} ] - E[Y_i^k ]^2)
= \frac{1}{n^2} \sum_{i=0}^n (\mu_{2k} - \mu_k^2) 
= \frac{\mu_{2k} - \mu_k^2}{n}
$$

(d) Propose an estimator of $\var[\hat{\mu}_k]$.

$$
\frac{\hat{\mu}_{2k} - \hat{\mu}_k^2}{n}  = \frac{\sum_{i=0}^n Y_i^{2k} - (\sum_{i=0}^n Y_i^{k})^2}{n}
$$

4.2 Calculate $E[(\bar{Y} - \mu)^3]$, the skewness of $\bar{y}$. Under what conditions is it zero?

...

4.3 Explain the difference between $\bar{Y}$ and $\mu$. Explain the difference between $n^{-1}\sum_{i=1}^n X_i X_i'$ and $E{X_i X_i'}$.

...

4.4 True or False. If $Y_i = X_i \beta + e_i, X_i \in \R, E[x_i|X_i] = 0$, and $\hat{e}_i$ is the OLS residual from the regression of $Y_i$ on $X_i$, then $\sum_{i=1}^nX_i^2 \hat{e}_i = 0$.

...

4.5  Prove (4.15) and (4.16).

(4.15) $E[\hat{\beta}|X] = \beta$

...

(4.16) $\var[\hat{\beta}|X] = (X'X)^{-1}(X' \Omega X) (X'X)^{-1}$

...

4.6 Prove Theorem 4.5.

Theorem 4.5 Generalized Gauss-Markov

In the linear regression model (Assumption 4.2) and $\Omega > 0$, if $\tilde{\beta}$ is a linear unbiased estimator of $\beta$ then $\var[\tilde{\beta}|X] \ge (X' \Omega^{-1}X)^{-1}$

...