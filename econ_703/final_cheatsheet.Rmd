---
title: "ECON 703 - Final Cheatsheet"
author: "Alex von Hafften"
date: "8/29/2020"
output: pdf_document
geometry: margin=1cm
header-includes:
- \AtBeginDocument{\let\maketitle\relax}
- \pagenumbering{gobble}
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \newcommand{\mtx}{\text{mtx}}
- \newcommand{\crd}{\text{crd}}
classoption:
- twocolumn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ECON 703 Final Cheatsheet

Let $X$ be a vector space and $T \in L(X, X)$. If $T(v) = \lambda v$, $\lambda$ is an \textbf{eigenvalue} of $T$ and $v \neq \bar{0}$ is an \textbf{eigenvector} corresp. to $\lambda$.

Let $W$ be a basis of $X$. $\lambda$ is an eigenvalue of $T$ iff $\lambda$ is an eigenvalue of $\mtx_W(T)$. $v$ is an eigenvector of $T$ corresp. to $\lambda$ iff $\crd_W(v)$ is an eigenvector of $\mtx_W(T)$ corresp. to $\lambda$.

If $\dim X = n$, $\mtx_W(T)$ is \textbf{diagonalizable} if $\exists$ basis $U$ s.t. $\mtx_U(T) = diag(\lambda_1, ..., \lambda_n)$. Thus, $\lambda_1, ..., \lambda_n$ are eigenvalues of $T$ and $U = \{u_1, ..., u_n\}$ are eigenvectors of $T$.

$\mtx_W(T)$ is diagonalizable $\iff$ eigenvectors of $T$ form a basis of $X$ $\iff$ eigenvectors of $\mtx_W(T)$ form a basis of $\R^n$.

If $\lambda_1, ..., \lambda_m$ are distinct eigenvalues of $T$ with corresponding eigenvectors $v_1, ..., v_m$, then $v_1, ..., v_m$ are linearly independent.

If $\dim X = n$ and $T$ has $n$ distinct eigenvalues, then $X$ has a basis consisting of $T$'s eigenvectors. Thus, if $W$ is a basis of $X$, $\mtx_W(T)$ is diagonalizable.

$A \in M_{n \times n}$ is \textbf{symmetric} if $a_{ij} = a_{ji}$ for all $i, j =1, ..., n$,

$A \in M_{n \times n}$ is \textbf{orthogonal} if $A^{-1} = A'$.

A basis $V= \{v_1, ..., v_n\}$ of $\R^n$ is \textbf{orthonormal} if $v_i \cdot v_j = 1$ when $i = j$ and $v_i \cdot v_j = 0$ when $i \neq j$.

A real $n \times n$ matrix $A$ is orthogonal iff $A$'s columns are orthonormal. (Thus, $A$'s columns = basis of $\R^n$.)

Let $T \in L(\R^n, \R^n)$ and $W$ be the standard basis of $\R^n$. If $\mtx_W(T)$ is symmetric, then

  - $T$ has $n$ eigenvalues.
  - $T$'s eigenvectors $\{v_1, ..., v_n\}$ are orthonormal basis of $\R^n$
  - $\mtx_W(T)$ is diagonaliable: $\mtx_W(T) = \mtx_{W, V}(id) \cdot \mtx_V(T) \cdot \mtx_{V,W}(id)$ ($\mtx_V(T)$ is diagonal and $\mtx_{W, V}(id), \mtx_{V,W}(id)$ are orthogonal)

\textbf{Quadratic Form}

$f(x_1, ..., x_n) = \sum_{i=1}^n \alpha_{ii}x_i^2+\sum_{i < j} \beta_{ij}x_ix_j\equiv x'Ax$,

$$
A = \begin{pmatrix} 
     \alpha_{11} & ... &  \alpha_{1n}\\
     \vdots & & \vdots \\
     \alpha_{n1} & ... &  \alpha_{nn}\\
     \end{pmatrix},
\alpha_{ij} = \begin{cases} 
              \beta_{ij}/2, i < j\\
              \beta_{ji}/2, i > j\\
              \end{cases}
$$

$A$ is symmetric $\implies A$ is diagonalizable, $A = U'DU$.

$A$'s eigenvectors $= V = \{v_1, ..., v_n\}$ are an orthonormal basis of $\R^n$.

$U = (v_1 ... v_n) = \mtx_{V, W}(id), W$ = standard basis of $\R^n$.

$\forall x \in \R^n$: $x = \sum_{i=1}^n \beta_i v_i, \beta_i = x \cdot v_i$

$f(x) = x'Ax = (\beta_1, ..., \beta_n) D (\beta_1, ..., \beta_n)^T = \sum_{i=1}^n \lambda_i \beta_i^2$, where $D =(\lambda_1, \lambda_2, ..., \lambda_n)$.

\begin{center}
\line(1,0){250}
\end{center}

Let $X$ be a vector space. A \textbf{norm} on $X$ is a function $|| \cdot ||:X \to \R_+$ s.t.

  - $||x|| \ge 0 \forall x \in X$
  - $||x|| = 0 \iff x = \bar{0}$
  - $||x+y|| \ge ||x|| + ||y|| \; \forall x, y \in X$
  - $||\alpha x|| = |\alpha| \cdot ||x|| \; \forall \alpha \in \R, x \in X$

A \textbf{normed vector space} is a vector space equipped with a norm.

Let $(X, || \cdot ||)$ be a normed vector space. Define $d: X \times X \to \R_+$ s.t. $d(x, y) = ||x - y||$. Then $(X, d)$ is a \textbf{metric space}.

For $X = \R^n$, $||x||_2 = \sqrt{\sum_{i=1}^n x_i^2}$, $||x||_1 = \sum_{i=1}^n |x_i|$, $||x||_\infty = \max\{|x_1|, ..., |x_n|\}$.
  
For $X = C([0,1])$, $||f||_2 = \sqrt{\int_0^1 f^2(t)dt}$, $||f||_1 = \int_0^1 |f(t)|dt$, $||f||_\infty = \sup_{t \in [0, 1]} |f(t)|$.
  
Suppose $X, Y$ are normed vector spaces and $T \in L(X, Y)$. We say that $T$ is \textbf{bounded} if $\exists \beta \in \R$ s.t. $||T(x)||_Y \le \beta ||x||_X$ $\forall x \in X$. $T$ is \textbf{bounded} is equivalent to:

  - $T$ is \textbf{continuous at} $x_0 \in X$.
  - $T$ is \textbf{continuous} $\forall x \in X$.
  - $T$ is \textbf{uniformly continuous}.
  - $T$ is \textbf{Lipschitz}.

Let $X, Y$ be normed vector spaces, $\dim X = n$. Then \textbf{every} $T \in L(X, Y)$ is \textbf{bounded}.

$B(X, Y) = \{ T \in L(X, Y) | T$ is bounded $\}$.  It $\dim X = n$, then $B(X, Y) \equiv L(X, Y)$. 

$||T||_{B(X, Y)} = \sup_{x \in X, x \neq \bar{0}} \Big \{\frac{||T(x)||_Y}{||x||_X} \Big \} = \sup_{||x||_X =1} \{||T(x)||_Y \}$

Working with $B(X, Y)$ instead of $L(X, Y)$ guarantees that $\sup$ exists.

$(B(X, Y), || \cdot ||_{B(X, Y)})$ is \textbf{normed vector space}.

\begin{center}
\line(1,0){250}
\end{center}

Let $f: I \to \R, I \subset\R$ is an open interval. $f$ is \textbf{differentiable at} $x \in I$ if $\lim_{h \to 0} \frac{f(x+h)-f(x)}{h} = a$ for some $a \in \R$.

Let $f: X \to \R, X \subset \R^n$ is an open set. $f$ is \textbf{differentiable at} $x \in X$ if $\lim_{h \to 0, h \in \R^n} \frac{|f(x+h)-(f(x)+a_1^x h_1+...+a_n^x h_n)|}{||h||} = 0$ for some $(a_1^x, ..., a_n^x) \in \R^n$.

$f$ is \text{differentiable} if it is differentiable at all $x \in X$.

Let $f: X \to \R^m, X \subset \R^n$ is an open set. $f$ is \textbf{differentiable at} $x \in X$ if $\lim_{h \to 0, h \in \R^n} \frac{||f(x+h)-(f(x)+A_xh)|}{||h||} = 0$ for some $A_x \in M_{m \times n}$. 

$\rightarrow f(x+h) \approx f(x) + A_x h$. Matrix $A_x =$ \textbf{Jacobian matrix}, denoted $Df(x)$.

Linear transformation $\R^n \to \R^m$ represented by $A_x =$ \textbf{differential}, denoted $df_x$. 

If f is differentiable at x, then its differential df_x is \textbf{unique}.

If f is differentiable at x, then $f$ is \textbf{continuous} at $x$.

$$
Df(x) = 
\begin{pmatrix}
\frac{\partial f^1}{\partial x_1} (x) & ... & \frac{\partial f^1}{\partial x_n} (x) \\
\vdots &   & \vdots \\
\frac{\partial f^m}{\partial x_1} (x) & ... & \frac{\partial f^m}{\partial x_n} (x) \\
\end{pmatrix}
$$

The partial derivative of $f$ is $\frac{\partial f^i}{\partial x_j} (x) := \lim_{\varepsilon \to 0} \frac{f^i(x+\varepsilon e_j)-f^i(x)}{\varepsilon}, i =1, ..., m, j=1, ..., n$.

\textbf{Chain Rule:} Let $X \subset \R^n, Y \subset \R^m$ be open, $f:X \to Y, g: Y \to \R^k$. Let $x_0 \in X$ and $F:= g \circ f$. If $f$ is differentiable at $x_0$ and $g$ is differentiable at $f(x_0)$, then $F$ is differentiable at $x_0$ and $dF_{x_0} = dg_{f(x_0)} \circ df_{x_0}$ and $DF(x_0)=Fg(f(x_0))Df(x_0)$.

\textbf{MVT}: Let $f: [a, b] \to \R$ be continuous $[a, b]$ and differential on $(a, b)$. Then there exists $c \in (a, b)$ such that $f(b)-f(a)=f'(c)(b-a)$.

\textbf{MVT}: Let $f: \R^n \to \R$ be differential on an open set $X \in \R^n, x, y \in X$, and $l(x, y):=\{\alpha x + (1-\alpha)y| \alpha \in [0,1]\} \subset X$. Then there exists $z \in l(x, y)$ such that $f(y) - f(x) = Df(z)(y-x)$.

\textbf{Rolle's Thm}: Let $f: [a, b] \to \R$ be continuous $[a, b]$ and differential on $(a, b)$. Assume that $f(a) = f(b) = 0$. Then there exists $c \in (a, b)$ such that $f'(c) = 0$.

\begin{center}
\line(1,0){250}
\end{center}

\textbf{Taylor's Thm}: Let $f: I \to \R$ is $n$ times differentiable with $I \subset \R$ is open and $[x, x+h] \subset I$. Then $f(x+h) = f(x) + \sum_{k=1}^{n-1}\frac{f^{(k)}(x)h^k}{k!}+\frac{f^{(n)}(x + \lambda h)h^n}{n!}, \lambda \in (0, 1)$ and  $f(x+h) = f(x) + \sum_{k=1}^{n}\frac{f^{(k)}(x)h^k}{k!}+o(h^n)$ as $h \to 0$.

If $f$ is $(n+1)$ times differentiable, then $f(x+h) = f(x) + \sum_{k=1}^{n}\frac{f^{(k)}(x)h^k}{k!}+O(h^{n+1})$ as $h \to 0$.

\textbf{Taylor's Thm}: Let $f: X \to \R^m$ is differentiable with $X \subset \R^n$ is open and $x \in X$. Then $f(x+h) = f(x) + Df(x)h+o(||h||)$ as $h \to 0$. If additionally, $f \in C^2$, then $f(x+h) = f(x) + Df(x)h+o(||h||^2)$ as $h \to 0$.

For $f: X \to \R, x \subset \R^n$, the \textbf{Hessian matrix} is

$$
D^2f(x) := 
\begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2}(x) & ... & \frac{\partial^2 f}{\partial x_n \partial x_1}(x) \\
\vdots & & \vdots \\
\frac{\partial^2 f}{\partial x_1 \partial x_n}(x) & ... & \frac{\partial^2 f}{\partial x_n^2} (x) \\
\end{pmatrix}
$$

If $f \in C^2$, then $D^2f(x)$ is symmetric.

\textbf{Taylor Thm}: Let $f: X \to \R$ is $C^2$ with $X \subset \R^n$ is open and $x \in X$. Then $f(x + h) = f(x) + Df(x)h+\frac{1}{2}h'D^2(x)h+o(||h||^2)$ as $h \to 0$. If additionally $f \in C^3$, then $f(x + h) = f(x) +Df(x)h+\frac{1}{2}h'D^2f(x)h+O(||h||^3)$ as $h \to 0$.

Let $f: X \to \R, X \in \R^n, f \in C^2$, then $D^2f(x)$ has eigenvalues $\lambda_1, ..., \lambda_n \in \R$. If $f$ has a local max/min at $x$, then $Df(x)=0$.  If $Df(x)=0$, then

  - $\lambda_1, ..., \lambda_n > 0 \implies f$ has a local minimum at $x$.
  - $\lambda_1, ..., \lambda_n < 0 \implies f$ has a local maximum at $x$.
  - $\exists i,j$ s.t. $\lambda_i>0, \lambda_j < 0 \implies f$ has a saddle point at $x$.
  - $\lambda_1, ..., \lambda_n \ge 0, \lambda_i>0$ for some $i \implies f$ has a local minimum or saddle at $x$.
  - $\lambda_1, ..., \lambda_n \le 0, \lambda_i<0$ for some $i \implies f$ has a local maximum or saddle at $x$.
  - $\lambda_1 = ... = \lambda_n = 0$ gives no information.

\textbf{Inverve Fn Thm}: Let $f: X \to \R^n$ be a continuously differentiable function, $X \subset \R^n$ be open, $x_0 \in X$. If $\det(Df(x_0)) \neq 0$, then there exists an open neighborhood $U$ of $x_0$ such that

  - $f$ is one-to-one in $U$
  - $V = f(U)$ is an open set, $y_0 :=f(x_0) \in V$
  - $f^{-1}$ is continuously differentiable and $Df^{-1}(y_0) = (Df(x_0))^{-1}$.

\textbf{Implicit Fn Thm}: Suppose $X \subset \R^n$ and $A \subset \R^p$ are open, $f: X \times A \to \R^n$ is continuously differentiable, $f(x_0, a_0) = 0$ and $\det(D_xf(x_0, a_0)) \neq 0$. Then there exist open neighborhoods $U$ of $x_0$ and $W$ of $a_0$ such that

  - $\forall a \in W \;\exists ! \equiv g(a) \in U$ s.t. $f(x, a) = f(g(a), a) = 0$
  - $g$ is continuously differentiable
  - $Dg(a_0)=-(D_xf(x_0, a_0))^{-1}D_a f(x_0, a_0)$

\begin{center}
\line(1,0){250}
\end{center}

A set $X \subset \R^n$ is \textbf{convex} if $\forall \lambda \in [0,1], x', x'' \in X$, the point $x_\lambda := (1-\lambda)x'+\lambda x'' \in X$

Any intersection of convex sets is convex.

If $X, Y$ are convex sets in $\R^n$, then for any $\alpha, \beta \in \R$, the set $z = \alpha X + \beta Y :=\{z \in \R^n| z = \alpha x + \beta y$ for some $x \in X, y \in Y \}$ is also convex.

A vector $p \neq \bar{0}$ in $\R^n$ and a scalar $\alpha \in \R$ define the \textbf{hyperplane} $H(p, \alpha)$ given by $H(p, \alpha) = \{ x \in \R^n | p \cdot x := \sum_{i=1}^n p_i x_i = \alpha\}$.

Vector $p$ is called the \textbf{normal} to the hyperplane $H(p, \alpha)$.

If $x', x'' \in H(p, \alpha), \lambda \in \R$, then $(1-\lambda)x' + \lambda x'' \in H(p, \alpha)$.

Sets $X$ and $Y$ are \textbf{separated} by a hyperplane $H(p, \alpha)$ if $p \cdot x \le \alpha p \le \cdot y$ $\forall x \in X, y \in Y$.

Sets $X$ and $Y$ are \textbf{strictly separated} by a hyperplane $H(p, \alpha)$ if $p \cdot x < \alpha <p \cdot y$ $\forall x \in X, y \in Y$.

A hyperplane $H(p, \alpha)$ \textbf{supports} a set $X$ if either $\alpha = \inf_{x \in X} (p \cdot x)$ and $\alpha = \sup_{x \in X} (p \cdot x)$

Let $X$ be a nonempty, closed, convex set in $\R^n, z \notin X$. Then

  - There exists $x^0 \in X$ and $H(p, \alpha)$ s.t. $x^0 \in H(p, \alpha), H(p, \alpha)$ supports $X$, and separates $X$ and $\{z\}$.
  - There exists a hyperplane $H(p, \beta)$ that strictly separates $X$ and $\{z\}$.
  
Let $X$ be a nonempty convex set in $\R^n, z \notin X$. Then there exists $H(p, \alpha)$ s.t., $z \in H(p, \alpha)$ and $H(p, \alpha)$ separates $X$ and $\{z\}$.

\textbf{SHT}: Let $X$ and $Y$ be disjoint and nonempty convex sets in $\R^n$. Then there exists a hyperplane $H(p, \alpha)$ that separates $X$ and $Y$.

Let $f: X \to X$. A point $x^* \in X$ is a \textbf{fixed point} of $f$ if $f(x^*) = x^*$.

Let $f:[a, b] \to [a, b]$ be continuous. Then $f$ has a \textbf{fixed point}.

\textbf{Brouwer's Fixed Point Theorem}: Let $X \subset \R^n$ be nonempty, compact, and convex, and let $f:X \to X$ be continuous. Then $f$ has a fixed point.


