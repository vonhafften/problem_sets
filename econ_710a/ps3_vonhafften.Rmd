---
title: "ECON 710A - Problem Set 3"
author: "Alex von Hafften^[I worked on this problem set with a study group of Michael Nattinger, Andrew Smith, and Ryan Mather. I also discussed problems with Sarah Bass, Emily Case, Danny Edgel, and Katherine Kwok.]"
date: "2/16/2020"
output: pdf_document
header-includes:
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \newcommand{\var}{\text{var}}
- \newcommand{\rank}{\text{rank}}
- \newcommand{\twiddle}{\tilde}
- \newcommand{\Lfn}{\mathcal{L}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(haven)
library(stargazer)
library(knitr)
library(varhandle)
library(matlib)
```

1. Let $(Y, X', Z')'$ be a random vector such that $Y = X'\beta + U$, $E[U|Z]=0$ where $E[ZX']$ is invertible and $E[Y^4 + ||X||^4 + ||Z||^4] < \infty$. Also, let $\{(Y_i, X_i', Z_i')'\}_{i=1}^n$ be a random sample from the distribution of $(Y, X', Z')'$.  We showed in lecture 4 that $\sqrt{n} (\hat{\beta}^{IV} - \beta) \to_d N(0, \Omega)$, $\Omega = E[ZX']^{-1}E[ZZ'U^2]E[XZ']^{-1}$.  Now suppose that $X = (X_1, X_2')'$, $Z = (Z_1, X_2')'$, and $E[U^2 | Z] = \sigma_U^2$ and let $\Omega_{11}$ be the upper left entry in $\Omega$.

(i) Show that $E[ZX']$ is invertible iff $E[ZZ']$ is invertible and $\pi_1 \neq 0$ where $(\pi_1, \pi_2')' = E[ZZ']^{-1}E[ZX_1]$.

Observe that

$$
E[ZX'] 
= E[\begin{pmatrix} Z_1 \\ X_2 \end{pmatrix}\begin{pmatrix} X_1 & X_2' \end{pmatrix}] \\
= \begin{pmatrix} E[Z_1X_1] & E[Z_1X_2'] \\ E[X_2X_1] & E[X_2X_2'] \end{pmatrix}
$$

$$
E[ZZ'] 
= E[\begin{pmatrix} Z_1 \\ X_2 \end{pmatrix}\begin{pmatrix} Z_1 & X_2' \end{pmatrix}] \\
= \begin{pmatrix} E[Z_1^2] & E[Z_1X_2'] \\ E[X_2Z_1] & E[X_2X_2'] \end{pmatrix}
$$

First, if either $E[ZZ']$ or $E[ZX']$ are invertible then, $E[X_2X_2']$ is invertible.  Assume for sake of a contradiction that $E[X_2X_2']$ is not invertible.  Then there exists some nonzero $t$ such that $E[X_2X_2']t=0 \implies  t'E[X_2X_2']t=t'0=0 \implies E[X_2't]=0 \implies E[Z_1X_2']t=0$.  This implies that $E[ZX']$ and $E[ZZ']$ are not invertible, which is a contradiction:

$$
E[ZX'](0, t')'
= \begin{pmatrix} E[Z_1X_1] & E[Z_1X_2'] \\ E[X_2X_1] & E[X_2X_2'] \end{pmatrix} \begin{pmatrix} 0 \\ t' \end{pmatrix} \\
=\begin{pmatrix} E[Z_1X_1](0) + E[Z_1X_2']t' \\ E[X_2X_1](0) + E[X_2X_2']t' \end{pmatrix}\\
= 0
$$

$$
E[ZZ'](0, t')'
= \begin{pmatrix} E[Z_1^2] & E[Z_1X_2'] \\ E[X_2Z_1] & E[X_2X_2'] \end{pmatrix} \begin{pmatrix} 0 \\ t' \end{pmatrix} \\
=\begin{pmatrix} E[Z_1^2](0) + E[Z_1X_2']t \\ E[X_2Z_1](0) + E[X_2X_2']t \end{pmatrix}\\
= 0
$$

By the block inversion formula, if $E[ZX']$ is invertible iff

$$
E[Z_1X_1] - E[Z_1X_2']E[X_2X_2']^{-1}E[X_2X_1] \neq 0
\iff E[\tilde{Z}_1X_1] \neq 0
$$

where $\tilde{Z}_1 := Z_1 - X_2'E[X_2X_2']^{-1}E[X_2Z_1]$. Denote $\pi_1 := \frac{E[\tilde{Z}_1X_1]}{E[\tilde{Z}_1^2]}$, 

$$
E[\tilde{Z}_1X_1] \neq 0 \iff \pi_1 E[\tilde{Z}_1^2] \neq 0 \\
\iff \pi_1 \neq 0 \text{ and } E[\tilde{Z}_1^2] = E[Z_1^2] - E[Z_1X_2']E[X_2X_2']^{-1}E[X_2Z_1] \neq 0
$$

By the block inversion formula, $E[Z_1^2] - E[Z_1X_2']E[X_2X_2']^{-1}E[X_2Z_1] \neq 0 \iff E[ZZ']$ is invertible. $\square$

\pagebreak

(ii) Show that $\Omega_{11} = \frac{\sigma_U^2}{E[\tilde{Z}_1^2]\pi_1^2}$ where $\tilde{Z}_1 = Z_1 - X_2'E[X_2X_2']^{-1}E[X_2Z_1]$.

Using the block inversion formula, define $A$ and $B$:

\begin{align*}
A 
&= E[ZX']^{-1}\\
&= \begin{pmatrix} E[Z_1X_1] & E[Z_1X_2'] \\ E[X_2X_1] & E[X_2X_2'] \end{pmatrix}^{-1} \\
B 
&= E[XZ']^{-1} \\
A_{11} 
&= B_{11} \\
&= (E[Z_1X_1] - E[X_2X_1]E[X_2X_2']^{-1}E[X_2X_1])^{-1} \\
&= E[\tilde{Z}_1 X_1]^{-1} \\
A_{12} 
&= B_{21} \\
&= -E[Z_1X_1]^{-1} E[Z_1X_2'] E[\tilde{Z}_1 X_1]^{-1} \\
A_{21} 
&= B_{12} \\
&= -E[\tilde{Z}_1 X_1]^{-1} E[X_2X_1] E[Z_1X_1]^{-1} \\
A_{22}
&= B_{22} \\
&= E[X_2X_2']^{-1} + E[X_2X_2']^{-1}E[X_2X_1] E[\tilde{Z}_1 X_1]^{-1}E[Z_1X_2']E[X_2X_2']^{-1}
\end{align*}

Homoskedastic variance-covariance matrix:

\begin{align*}
\Omega 
&= \sigma_U^2E[ZX']^{-1}E[ZZ']E[XZ']^{-1} \\
&= \sigma_U^2AE[ZZ']B \\
&= \sigma_U^2 
\begin{pmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{pmatrix}
\begin{pmatrix} E[Z_1^2] & E[Z_1X_2'] \\ E[X_2Z_1] & E[X_2X_2'] \end{pmatrix}
\begin{pmatrix} B_{11} & B_{12} \\ B_{21} & B_{22} \end{pmatrix}\\
&= \sigma_U^2 
\begin{pmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{pmatrix}
\begin{pmatrix} 
B_{11}E[Z_1^2]+B_{21}E[Z_1X_2'] & 
B_{12}E[Z_1^2]+B_{22}E[Z_1X_2'] \\ 
B_{11}E[X_2Z_1] + B_{21}E[X_2X_2'] & 
B_{12}E[X_2Z_1] + B_{22}E[X_2X_2'] 
\end{pmatrix}\\
&= \sigma_U^2  
\begin{pmatrix} 
A_{11}B_{11}E[Z_1^2]+A_{11}B_{21}E[Z_1X_2'] + A_{12}B_{11}E[X_2Z_1] + A_{12}B_{21}E[X_2X_2'] & 
... \\ 
... & 
...
\end{pmatrix}
\end{align*}

\begin{align*}
\Omega_{11}
&= \sigma_U^2 [A_{11}B_{11}E[Z_1^2]+A_{11}B_{21}E[Z_1X_2'] + A_{12}B_{11}E[X_2Z_1] + A_{12}B_{21}E[X_2X_2']] \\ 
&= \sigma_U^2 [E[\tilde{Z}_1 X_1]^{-1}E[\tilde{Z}_1 X_1]^{-1}E[Z_1^2] \\
&+ E[\tilde{Z}_1 X_1]^{-1}( -E[Z_1X_1]^{-1} E[Z_1X_2'] E[\tilde{Z}_1 X_1]^{-1})E[Z_1X_2'] \\
&+ ( -E[Z_1X_1]^{-1} E[Z_1X_2'] E[\tilde{Z}_1 X_1]^{-1})E[\tilde{Z}_1 X_1]^{-1}E[X_2Z_1] \\
&+ ( -E[Z_1X_1]^{-1} E[Z_1X_2'] E[\tilde{Z}_1 X_1]^{-1})( -E[Z_1X_1]^{-1} E[Z_1X_2'] E[\tilde{Z}_1 X_1]^{-1})E[X_2X_2']] \\
&= \frac{\sigma_U^2}{E[\tilde{Z}_1^2]}\frac{E[\tilde{Z}_1^2]^2}{E[\tilde{Z}_1X_1]^2} \\
&= \frac{\sigma_U^2}{E[\tilde{Z}_1^2]\pi_1^2}
\end{align*}

\pagebreak

(iii) Explain, using "regression language", what $(\pi_1, \pi_2')'$ and $\tilde{Z}_1$ is.

$(\pi_1, \pi_2')'$ are the coefficients of the projection of $X_1$ onto $Z$.  $\tilde{Z}_1$ is the regression error from the projection of $Z_1$ on $X_2$.

(iv) Show that $\Omega_{11} \ge \frac{\sigma_U^2}{E[Z_*^2]}$ where $Z_* = E[X_1 | Z] - X_2' E[X_2X_2']^{-1}E[X_2E[X_1 | Z]]$ and provide restrictions on $E[X_1 | Z]$ such that $\Omega_{11} = \frac{\sigma_U^2}{E[Z_*^2]}$.

First, note that 

\begin{align*}
E[\tilde{Z}_1 X_2] 
&= E[X_2 \tilde{Z}_1] \\
&= E[X_2(Z_1 - X_2'E[X_2X_2']^{-1}E[X_2Z_1])] \\
&= E[X_2Z_1 - X_2X_2'E[X_2X_2']^{-1}E[X_2Z_1]] \\
&= E[X_2Z_1] - E[X_2X_2']E[X_2X_2']^{-1}E[X_2Z_1] \\
&= E[X_2Z_1] - E[X_2Z_1] \\
&= 0
\end{align*}

This implies:

\begin{align*}
E[\tilde{Z}_1 X_2] &= 0 \\
\implies E[\tilde{Z}_1 X_2] E[X_2X_2']^{-1} E[X_2X_2'] &= 0 \\
\implies E[\tilde{Z}_1 X_2 E[X_2X_2']^{-1} E[X_2E[X_1|Z]]] &= 0
\end{align*}

\begin{align*}
E[\tilde{Z}_1 X_1]  
&= E[\tilde{Z}_1 E[X_1|Z]] \\
&= E[\tilde{Z}_1 E[X_1|Z]] + E[\tilde{Z}_1 X_2 E[X_2X_2']^{-1} E[X_2E[X_1|Z]]]\\
&= E[\tilde{Z}_1 (E[X_1|Z] + X_2 E[X_2X_2']^{-1} E[X_2E[X_1|Z]])]\\
&= E[\tilde{Z}_1 Z_*]
\end{align*}

By Cauchy-Schwarz,

\begin{align*}
\Omega_{11} 
&= \frac{\sigma_U^2}{E[\tilde{Z}_1^2]\pi_1^2} \\
&= \frac{\sigma_U^2E[\tilde{Z}_1^2]}{E[\tilde{Z}_1X_1]^2} \\
&= \frac{\sigma_U^2E[\tilde{Z}_1^2]}{E[\tilde{Z}_1Z_*]^2} \\
&\ge \frac{\sigma_U^2E[\tilde{Z}_1^2]}{E[\tilde{Z}_1^2]E[Z_*^2]} \\
&= \frac{\sigma_U^2}{E[Z_*^2]}
\end{align*}

\pagebreak

The restriction on $E[X_1 | Z]$:

\begin{align*}
E[X_1 | Z] 
&= Z_1 \pi_1 + X_2' \pi_2 \\
\implies Z_* 
&= Z_1 \pi_1 + X_2' \pi_2 - X_2' E[X_2X_2']^{-1}E[X_2(Z_1 \pi_1 + X_2' \pi_2)]\\
&= Z_1 \pi_1 + X_2' \pi_2 - X_2' E[X_2X_2']^{-1}E[X_2Z_1] \pi_1 - X_2' E[X_2X_2']^{-1}E[X_2X_2'] \pi_2\\
&= Z_1 \pi_1 + X_2' \pi_2 - X_2' E[X_2X_2']^{-1}E[X_2Z_1] \pi_1 - X_2' \pi_2 \\
&= (Z_1 - X_2' E[X_2X_2']^{-1}E[X_2Z_1]) \pi_1 \\
&= \tilde{Z}_1 \pi_1 \\
\implies 
\frac{\sigma_U^2}{E[Z_*^2]}
&= \frac{\sigma_U^2}{E[(\tilde{Z}_1 \pi_1)^2]} \\
&= \frac{\sigma_U^2}{E[\tilde{Z}_1^2 \pi_1^2]} \\
&= \frac{\sigma_U^2}{E[\tilde{Z}_1^2 ]\pi_1^2} \\
&= \Omega_{11}
\end{align*}

(v) Suppose that $X_2 = 1$. Write $\Omega_{11}/\sigma_U^2$ as a function of variances and covariances involving $Z_1$ and $X_1$.

\begin{align*}
X_2 
&= 1 \\
\implies
Z_1 - (1)E[(1)(1)]^{-1}E[(1)Z_1] 
&= Z_1 - E[Z_1] \\
\implies
\Omega_{11}
&= \frac{\sigma_U^2E[\tilde{Z}_1^2 ]}{E[\tilde{Z}_1 X_1]^2} \\
&= \frac{\sigma_U^2E[(Z_1 - E[Z_1])^2 ]}{E[(Z_1 - E[Z_1])X_1 ]^2} \\
&= \frac{\sigma_U^2 Var[Z_1]}{Cov[Z_1, X_1 ]^2} 
\end{align*}

\pagebreak

2. Let $(Y, X, Z)'$ be a random vector such that $Y = X \beta_1 + U, E[U|Z] = 0$ where $E[Y_4+X_4+Z_4]<\infty$. Also, let $\{(Y_i,X_i,Z_i)'\}^n_{i=1}$ be a random sample from the distribution of $(Y,X',Z')'$. Let $h$ be a function of $Z$ such that $E[h(Z)^4]<\infty$.

(i) Provide conditions such that $E[h(Z)(Y - X \beta)] = 0$ iff $\beta = \beta_1$.

On the condition that $E[h(Z)X] \neq 0$:

\begin{align*}
E[h(Z)(Y - X \beta)]
&= E[h(Z)(X \beta_1 + U - X \beta)] \\
&= E[h(Z)U] + E[h(Z)X (\beta_1 - \beta) ] \\
&= E[h(Z)E[U|Z]] + E[h(Z)X] (\beta_1 - \beta) \\
&= E[h(Z)X] (\beta_1 - \beta)\\
\iff \beta 
&= \beta_1
\end{align*}

(ii) Derive a method of moments estimator of $\beta_1$, say $\hat{\beta}_1^h$, using the IV moment in (i).

The method of moments estimator is:

\begin{align*}
\frac{1}{n}\sum_{i = 1}^{n}h(Z_i)(Y_i - X_i \hat{\beta}_1^h) 
&= 0 \\
\implies
\frac{1}{n}\sum_{i = 1}^{n}h(Z_i)Y_i - \frac{1}{n}\sum_{i = 1}^{n}h(Z_i) X_i \hat{\beta}_1^h 
&= 0 \\
\implies
\hat{\beta}_1^h 
&=  \frac{\frac{1}{n} \sum_{i = 1}^{n}h(Z_i)Y_i}{\frac{1}{n}\sum_{i = 1}^{n}h(Z_i) X_i}  \\
&=  \frac{\frac{1}{n} \sum_{i = 1}^{n}h(Z_i)(X_i \beta_1 + U_i)}{\frac{1}{n}\sum_{i = 1}^{n}h(Z_i) X_i}  \\
&=  \frac{\frac{1}{n} \sum_{i = 1}^{n}h(Z_i)X_i \beta_1}{\frac{1}{n}\sum_{i = 1}^{n}h(Z_i) X_i} 
+ \frac{\frac{1}{n} \sum_{i = 1}^{n}h(Z_i)U_i}{\frac{1}{n}\sum_{i = 1}^{n}h(Z_i) X_i}\\
&=  \beta_1
+ \frac{\frac{1}{n} \sum_{i = 1}^{n}h(Z_i)U_i}{\frac{1}{n}\sum_{i = 1}^{n}h(Z_i) X_i}
\end{align*}

(iii) Under the conditions provided in (i), show that $\sqrt{n}(\hat{\beta}_1^h - \beta_1) \to_d N(0, \Omega^h)$ for some asymptotic variance $\Omega^h \ge 0$.

$$
\sqrt{n}(\hat{\beta}_1^h - \beta_1)
= \frac{\frac{1}{\sqrt{n}} \sum_{i = 1}^{n}h(Z_i)U_i}
{\frac{1}{n}\sum_{i = 1}^{n}h(Z_i) X_i}
$$

Because $E[h(Z) X]^2 < E[h(Z)^2]E[X^2] < \infty$, the law of large numbers implies that

$$
\frac{1}{n}\sum_{i = 1}^{n} h(Z_i) X_i \to_p E[h(Z) X]
$$

By the condition from (i), $E[h(Z) X] \neq 0$.  Because $E[h(Z)^2U^2]^2 \le E[h(Z)^4]E[U^4] < \infty$, the central limit theorem implies that

$$
\frac{1}{\sqrt{n}} \sum_{i = 1}^{n}h(Z_i)U_i \to_d N(0, E[h(Z)^2U^2])
$$

Finally, by the continuous mapping theorem,

$$
\sqrt{n}(\hat{\beta}_1^h - \beta_1) \\
= \frac{\frac{1}{\sqrt{n}} \sum_{i = 1}^{n}h(Z_i)U_i}
{\frac{1}{n}\sum_{i = 1}^{n}h(Z_i) X_i} \\
\to_d N(0, \frac{E[h(Z)^2U^2]}{E[h(Z)X]^2})
$$


(iv) Show that $\Omega^h \ge E[\frac{E[X|Z]^2}{E[U^2|Z]}]^{-1}$ and find a function $h$, such that $\Omega^h$ achieves this lower bound.

From (iii),

\begin{align*}
\Omega^h 
&= \frac{E[h(Z)^2U^2]}{E[h(Z)X]^2}  \\
&= \frac{E[h(Z)^2E[U^2|Z]]}{E[h(Z)E[X|Z]]^2} \\
&= \frac{E[h(Z)^2E[U^2|Z]]}
{E[h(Z) \sqrt{E[U^2|Z]}\frac{E[X|Z]}{\sqrt{E[U^2|Z]}}]^2} \\
&\ge \frac{E[h(Z)^2E[U^2|Z]]}
{E[h(Z)^2 E[U^2|Z]] E[\frac{E[X|Z]^2}{E[U^2|Z]}]} \\
&= [ E[\frac{E[X|Z]^2}{E[U^2|Z]}]]^{-1} \\
\end{align*}

If $h(Z) = \frac{E[X|Z]}{E[U^2|Z]}$:

\begin{align*}
\Omega_h
&= \frac{E[[\frac{E[X|Z]}{E[U^2|Z]}]^2U^2]}{E[\frac{E[X|Z]}{E[U^2|Z]}X]^2} \\
&= [ E[\frac{E[X|Z]^2}{E[U^2|Z]}]]^{-1}
\end{align*}

\pagebreak

3. Consider the data from Angrist and Krueger (1991) provided on the course website and the following linear model for $log(wage)$ as a function of educationand additional control variables: $log(wage) = \beta_0 + educ \cdot \beta_1 + \sum_{t=31}^{39} 1\{yob = t\} \beta_t + \sum_{s=1}^{50} 1\{sob = s\} \gamma_s + U$, where $yob$ is year of birth and $sob$ is state of birth. As instruments for $educ$ consider three instruments: $1\{qob= 2\},1\{qob= 3\},1\{qob= 4\}$ where $qob$ is quarter of birth. Using matrix algebra and your preferred statistical software, write code that loads the data and computes the 2SLS estimate of $\beta_1$ and the heteroskedasticity robust standard error stemming from the variance estimator formula (12.40) on page 354 of Bruce Hansenâ€™s textbook.  The solution to this exercise should include code and the two numbers produced by it.

```{r problem3}
data <- read_csv("AK91.csv", col_types = "ddddd")

n <- nrow(data)

# prep variables
y <- data$lwage
x_1 <- data$educ
controls <- cbind(rep(1, n),
                  to.dummy(data$yob, prefix = "yob")[,2:10],
                  to.dummy(data$sob, prefix = "sob")[,2:51])
instruments <- to.dummy(data$qob, prefix = "qob")[,2:4]

z <- cbind(instruments, controls)
x <- cbind(x_1, controls)

k <- ncol(x)
l <- ncol(z)

# Estimating 2sls beta using 12.29 in Hansen
pi_hat <- solve(t(z) %*% z) %*% t(z) %*% x_1
z_2 <- cbind(z %*% pi_hat, controls)
beta_2sls <- solve(t(z_2) %*% x) %*% t(z_2) %*% y

# Estimating heteroskedastic robust standard errors using 12.40 in Hansen
e_hat <- as.numeric(y - x %*% beta_2sls)

omega <- crossprod(z_2 * e_hat)

varcov <- solve(t(z_2) %*% x) %*% omega %*% solve(t(x) %*% z_2)

print(beta_2sls[1])
print(sqrt(varcov[1,1]))
```