---
title: "ECON 710A - Problem Set 5"
author: "Alex von Hafften^[I worked on this problem set with a study group of Michael Nattinger, Andrew Smith, and Ryan Mather. I also discussed problems with Sarah Bass, Emily Case, Danny Edgel, and Katherine Kwok.]"
date: "3/1/2021"
output: pdf_document
header-includes:
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \newcommand{\var}{\text{var}}
- \newcommand{\rank}{\text{rank}}
- \newcommand{\twiddle}{\tilde}
- \newcommand{\Lfn}{\mathcal{L}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(knitr)
```

1. Suppose that $\{\varepsilon_t\}_{t=0}^T$ are iid random variables with mean zero, variance $\sigma^2$ and $E[\varepsilon_t^8] < \infty$. Let $U_t = \varepsilon_t \varepsilon_{t-1}$, $W_t = \varepsilon_t \varepsilon_0$, and $V_t = \varepsilon_t^2 \varepsilon_{t-1}$ where $t=1, ..., T$.

(i) Show that $\{U_t\}_{t=1}^T$, $\{W_t\}_{t=1}^T$, and $\{V_t\}_{t=1}^T$ are covariance stationary.

For each time series, we check that (1) the second moment is finite, (2) the mean does not depend on $t$, and (3) the variance does not depend on $t$.

$\{U_t\}_{t=1}^T$: For (1), because $E[\varepsilon_t^8] < \infty$ and $\{\varepsilon_t\}_{t=0}^T$ are iid,

\begin{align*}
E[U_t^2] 
&= E[(\varepsilon_t \varepsilon_{t-1})^2]\\
&= E[\varepsilon_t^2 \varepsilon_{t-1}^2]\\
&= E[\varepsilon_t^2] E[\varepsilon_{t-1}^2]\\
&= E[\varepsilon_t^2]^2\\
&= \sigma^4\\
&< \infty
\end{align*}

For (2), $E[U_t] = E[\varepsilon_t \varepsilon_{t-1}] = E[\varepsilon_t]E[\varepsilon_{t-1}] = 0$.  For (3), 

\begin{align*}
\gamma(0) 
&= Cov(U_t, U_t)\\
&= Var(U_t)\\
&= Var(\varepsilon_t \varepsilon_{t-1})\\
&= Var(\varepsilon_t)Var( \varepsilon_{t-1})\\
&= \sigma^4
\end{align*}

\begin{align*}
\gamma(1) 
&= Cov(U_t, U_{t+1})\\
&= E[U_tU_{t+1}]\\
&= E[(\varepsilon_t\varepsilon_{t-1})(\varepsilon_{t+1}\varepsilon_{t})]\\
&= E[\varepsilon_t^2]E[\varepsilon_{t-1}]E[\varepsilon_{t+1}]\\
&= 0
\end{align*}

\begin{align*}
\gamma(2) 
&= Cov(U_t, U_{t+2}) \\
&= E[U_tU_{t+2}]\\
&= E[(\varepsilon_t\varepsilon_{t-1})(\varepsilon_{t+2}\varepsilon_{t+1})]\\
&= E[\varepsilon_{t-1}]E[\varepsilon_t]E[\varepsilon_{t+1}]E[\varepsilon_{t+2}]\\
&= 0
\end{align*}

Thus, $\gamma(k) = \sigma^4$ if $k = 0$ and zero otherwise.

$\{W_t\}_{t=1}^T$: For (1), because $E[\varepsilon_t^8] < \infty$ and $\{\varepsilon_t\}_{t=0}^T$ are iid,

\begin{align*}
E[W_t^2] 
&= E[(\varepsilon_t \varepsilon_0)^2]\\
&= E[\varepsilon_t^2 \varepsilon_0^2]\\
&= E[\varepsilon_t^2] E[\varepsilon_0^2]\\
&= E[\varepsilon_t^2]^2\\
&= \sigma^4\\
&< \infty
\end{align*}

For (2), $E[W_t] = E[\varepsilon_t \varepsilon_0] = E[\varepsilon_t]E[\varepsilon_0] = 0$.  For (3), 

\begin{align*}
\gamma(0) 
&= Cov(W_t, W_t)\\
&= Var(W_t)\\
&= Var(\varepsilon_t \varepsilon_0)\\
&= Var(\varepsilon_t)Var( \varepsilon_0)\\
&= \sigma^4
\end{align*}

\begin{align*}
\gamma(1) 
&= Cov(W_t, W_{t+1})\\
&= E[W_tW_{t+1}]\\
&= E[(\varepsilon_t\varepsilon_0)(\varepsilon_{t+1}\varepsilon_0)]\\
&= E[\varepsilon_0^2]E[\varepsilon_t]E[\varepsilon_{t+1}]\\
&= 0
\end{align*}

\begin{align*}
\gamma(2) 
&= Cov(W_t, W_{t+2})\\
&= E[W_tW_{t+2}]\\
&= E[(\varepsilon_t\varepsilon_0)(\varepsilon_{t+2}\varepsilon_0)]\\
&= E[(\varepsilon_t\varepsilon_0)(\varepsilon_{t+2}\varepsilon_0)]\\
&= E[\varepsilon_0^2]E[\varepsilon_t]E[\varepsilon_{t+2}]\\
&= 0
\end{align*}

Thus, $\gamma(k) = \sigma^4$ if $k = 0$ and zero otherwise.

$\{V_t\}_{t=1}^T$: For (1), because $E[\varepsilon_t^8] < \infty$ and $\{\varepsilon_t\}_{t=0}^T$ are iid,

\begin{align*}
E[V_t^2] 
&= E[(\varepsilon_t^2 \varepsilon_{t-1})^2]\\
&= E[\varepsilon_t^4 \varepsilon_{t-1}^2]\\
&= E[\varepsilon_t^4] E[\varepsilon_{t-1}^2]\\
&= E[\varepsilon_t^4] \sigma^2\\
&< \infty
\end{align*}

For (2), $E[V_t] = E[\varepsilon_t^2 \varepsilon_{t-1}] = E[\varepsilon_t^2]E[\varepsilon_{t-1}] = 0$.  For (3), 

\begin{align*}
\gamma(0) 
&= Cov(V_t, V_t)\\
&= Var(V_t)\\
&= Var(\varepsilon_t^2 \varepsilon_{t-1})\\
&= Var(\varepsilon_t^2) Var( \varepsilon_{t-1})\\
&= E[(\varepsilon_t^2-E[\varepsilon_t^2])^2] \sigma^2\\
&= E[(\varepsilon_t^2-\sigma^2)^2] \sigma^2\\
&= E[\varepsilon_t^4-2\sigma^2\varepsilon_t^2 + \sigma^4] \sigma^2\\
&= (E[\varepsilon_t^4]-2\sigma^2\sigma^2 + \sigma^4) \sigma^2 \\
&= (E[\varepsilon_t^4] - \sigma^4) \sigma^2 \\
&= \sigma^2 E[\varepsilon_t^4] - \sigma^6
\end{align*}

\begin{align*}
\gamma(1) 
&= Cov(V_t, V_{t+1})\\
&= E[V_tV_{t+1}]\\
&= E[(\varepsilon_t^2\varepsilon_{t-1})(\varepsilon_{t+1}^2\varepsilon_{t})]\\
&= E[\varepsilon_t^3\varepsilon_{t-1}\varepsilon_{t+1}^2]\\
&= E[\varepsilon_t^3]E[\varepsilon_{t-1}]E[\varepsilon_{t+1}^2]\\
&= 0
\end{align*}

\begin{align*}
\gamma(2) 
&= Cov(V_t, V_{t+2})\\
&= E[V_tV_{t+2}]\\
&= E[(\varepsilon_t^2\varepsilon_{t-1})(\varepsilon_{t+2}^2\varepsilon_{t+1})]\\
&= E[\varepsilon_t^2\varepsilon_{t-1}\varepsilon_{t+2}^2\varepsilon_{t+1}]\\
&= E[\varepsilon_t^2]E[\varepsilon_{t-1}]E[\varepsilon_{t+2}^2]E[\varepsilon_{t+1}]\\
&= 0
\end{align*}

Thus, $\gamma(k) = \sigma^2 E[\varepsilon_t^4] - \sigma^6$ if $k = 0$ and zero otherwise.

\pagebreak

(ii) Argue that the following three sample means $\bar{U}$, $\bar{W}$, $\bar{V}$ converge in probability to their expectations.

In (i), we found that $E[U_t]=E[W_t]=E[V_t]=0 \implies E[\bar{U}]=E[\bar{W}]=E[\bar{V}]=0$.  Below I show that $Var(\bar{U}) \to 0$, $Var(\bar{V}) \to 0$, and $Var(\bar{W}) \to 0$, so by Chebyshev's inequality $\bar{U} \to_p E[\bar{U}]$, $\bar{W} \to_p E[\bar{W}]$, and $\bar{V} \to_p E[\bar{V}]$.

\begin{align*}
Var(\bar{U}) 
&= Var\Bigg(\frac{1}{T}\sum_{t=1}^T U_t \Bigg)\\
&= \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T Cov(U_t, U_s)\\
&= \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T \gamma(t-s)\\
&= \frac{1}{T^2} T \gamma(0)\\
&= \frac{\gamma(0)}{T} \\
&= \frac{\sigma^2}{T} \\
&\to 0
\end{align*}

As $T \to \infty$.  Because $V_t$ and $W_t$ have the same autocovariance function, the variances of $\bar{W}$ and $\bar{V}$ similarly converge to zero.

(iii) Determine whether the following three sample second moments converge in probability to their expectations:

$$
\hat{\gamma}_U(0) = \frac{1}{T} \sum_{t=1}^T U_t^2, \; \; \;
\hat{\gamma}_W(0) = \frac{1}{T} \sum_{t=1}^T W_t^2,  \; \; \;
\hat{\gamma}_V(0) = \frac{1}{T} \sum_{t=1}^T V_t^2
$$

Similar to (ii), we proceed by applying Chebyshev's inequality to show convergence.  For $\hat{\gamma}_U(0)$,

$$
E[\hat{\gamma}_U(0)] = E[\frac{1}{T} \sum_{t=1}^T U_t^2]=\frac{1}{T} \sum_{t=1}^T E[U_t^2]=\sigma^4
$$

Now, let us consider the autocorrelation function for $\{U_t^2\}_{t=0}^T$:

\begin{align*}
\gamma_{U^2}(0) 
&= Var(U_t^2)\\
&= E[U_t^4]-(\sigma^4)^2\\
&= E[\varepsilon_t^4\varepsilon_{t-1}^4]-\sigma^8\\
&= E[\varepsilon_t^4]E[\varepsilon_{t-1}^4]-\sigma^8\\
&= E[\varepsilon_t^4]^2-\sigma^8
\end{align*}

\begin{align*}
\gamma_{U^2}(1) 
&= Cov(U_t^2, U_{t+1}^2)\\
&=E[U_t^2U_{t+1}^2] - E[U_t^2]E[U_{t+1}^2]\\
&=E[(\varepsilon_t\varepsilon_{t-1})^2(\varepsilon_{t+1}\varepsilon_{t})^2] - \sigma^4\sigma^4\\
&= E[\varepsilon_t^4\varepsilon_{t-1}^2\varepsilon_{t+1}^2] - \sigma^8\\
&= E[\varepsilon_t^4]\sigma^2\sigma^2-\sigma^8\\
&= E[\varepsilon_t^4]\sigma^4-\sigma^8
\end{align*}

\begin{align*}
\gamma_{U^2}(2) 
&= Cov(U_t^2, U_{t+2}^2)\\
&=E[U_t^2U_{t+2}^2] - E[U_t^2]E[U_{t+2}^2]\\
&=E[(\varepsilon_t\varepsilon_{t-1})^2 (\varepsilon_{t+2}\varepsilon_{t+1})^2] - \sigma^8\\
&=E[\varepsilon_t^2]E[\varepsilon_{t-1}^2]E[ \varepsilon_{t+2}^2]E[\varepsilon_{t+1}^2] - \sigma^8\\
&=(\sigma^2)^4 - \sigma^8\\
&=0
\end{align*}

Therefore,

\begin{align*}
Var\Bigg(\frac{1}{T}\sum_{t=1}^T U_t^2 \Bigg)
&= \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T Cov(U_t^2, U_s^2)\\
&= \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T \gamma_{U^2}(t-s)\\
&= \frac{1}{T^2} T (E[\varepsilon_t^4]^2-\sigma^8 +E[\varepsilon_t^4]\sigma^4-\sigma^8)\\
&= \frac{E[\varepsilon_t^4]^2-2\sigma^8 +E[\varepsilon_t^4]\sigma^4}{T}\\
&\to 0
\end{align*}

As $T \to \infty$. For $\hat{\gamma}_W(0)$,

$$
E[\hat{\gamma}_W(0)] = E[\frac{1}{T} \sum_{t=1}^T W_t^2]=\frac{1}{T} \sum_{t=1}^T E[W_t^2]=\sigma^4
$$

Now, let us consider the autocorrelation function for $\{W_t^2\}_{t=0}^T$:

\begin{align*}
\gamma_{W^2}(0) 
&= Var(W_t^2)\\
&= E[W_t^4]-(\sigma^4)^2\\
&= E[\varepsilon_t^4\varepsilon_0^4]-\sigma^8\\
&= E[\varepsilon_t^4]E[\varepsilon_0^4]-\sigma^8\\
&= E[\varepsilon_t^4]^2-\sigma^8
\end{align*}

\begin{align*}
\gamma_{W^2}(1) 
&= Cov(W_t^2, W_{t+1}^2)\\
&=E[W_t^2W_{t+1}^2] - E[W_t^2]E[W_{t+1}^2]\\
&=E[(\varepsilon_t\varepsilon_0)^2(\varepsilon_{t+1}\varepsilon_0)^2] - \sigma^4\sigma^4\\
&= E[(\varepsilon_t^2\varepsilon_{t+1}^2\varepsilon_{0}^4] - \sigma^8\\
&= E[\varepsilon_0^4]\sigma^2\sigma^2-\sigma^8\\
&= E[\varepsilon_t^4]\sigma^4-\sigma^8
\end{align*}

\begin{align*}
\gamma_{W^2}(2) 
&= Cov(W_t^2, W_{t+2}^2)\\
&=E[W_t^2 W_{t+2}^2] - E[W_t^2]E[W_{t+2}^2]\\
&=E[(\varepsilon_t\varepsilon_0)^2 (\varepsilon_{t+2}\varepsilon_0)^2] - \sigma^8\\
&=E[\varepsilon_t^2]E[\varepsilon_{t+2}^2]E[\varepsilon_0^4] - \sigma^8\\
&=E[\varepsilon_t^4]\sigma^4 - \sigma^8
\end{align*}

Thus, for $k \ge 2$, $\gamma_{W^2}(k) > 0$, so $\hat{\gamma}_W(0)$ does not converge to its expectation.

For $\hat{\gamma}_V(0)$,

$$
E[\hat{\gamma}_V(0)] = E[\frac{1}{T} \sum_{t=1}^T V_t^2]=\frac{1}{T} \sum_{t=1}^T E[V_t^2]=\sigma^2E[\varepsilon_t^4]
$$

Now, let us consider the autocorrelation function for $\{V_t^2\}_{t=0}^T$:

\begin{align*}
\gamma_{V^2}(0) 
&= Var(V_t^2)\\
&= E[V_t^4]-E[V_t^2]^2\\
&= E[(\varepsilon_t^2\varepsilon_{t-1})^4]-\sigma^4E[\varepsilon_t^4]^2\\
&= E[\varepsilon_t^8\varepsilon_{t-1}^4]-\sigma^4E[\varepsilon_t^4]^2\\
&= E[\varepsilon_t^8]E[\varepsilon_{t}^4]-\sigma^4E[\varepsilon_t^4]^2
\end{align*}

\begin{align*}
\gamma_{V^2}(1)
&= Cov(V_t^2, V_{t+1}^2)\\
&= E[V_t^2V_{t+1}^2] - E[V_t^2]E[V_{t+1}^2]\\
&= E[(\varepsilon_t^2 \varepsilon_{t-1})^2 (\varepsilon_{t+1}^2 \varepsilon_t)^2] -
\sigma^2E[\varepsilon_t^4]\sigma^2E[\varepsilon_t^4]\\
&= E[\varepsilon_t^6 \varepsilon_{t-1}^2 \varepsilon_{t+1}^4] -
\sigma^4E[\varepsilon_t^4]^2\\
&= E[\varepsilon_t^6] E[\varepsilon_t^4] \sigma^2  -
\sigma^4E[\varepsilon_t^4]^2\\
\end{align*}

\begin{align*}
\gamma_{V^2}(1)
&= Cov(V_t^2, V_{t+2}^2)\\
&= E[V_t^2V_{t+2}^2] - E[V_t^2]E[V_{t+2}^2]\\
&= E[(\varepsilon_t^2 \varepsilon_{t-1})^2 (\varepsilon_{t+2}^2 \varepsilon_{t+1})^2] -
\sigma^2E[\varepsilon_t^4]\sigma^2E[\varepsilon_t^4]\\
&= E[\varepsilon_t^4 \varepsilon_{t-1}^2 \varepsilon_{t+2}^4 \varepsilon_{t+1}^2] -
\sigma^4E[\varepsilon_t^4]^2\\
&= E[\varepsilon_t^4]E[ \varepsilon_{t-1}^2 ]E[\varepsilon_{t+2}^4 ]E[\varepsilon_{t+1}^2] -
\sigma^4E[\varepsilon_t^4]^2\\
&= 0
\end{align*} 

Therefore,

\begin{align*}
Var\Bigg(\frac{1}{T}\sum_{t=1}^T V_t^2 \Bigg)
&= \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T Cov(V_t^2, V_s^2)\\
&= \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T \gamma_{V^2}(t-s)\\
&= \frac{1}{T^2} T (E[\varepsilon_t^8]E[\varepsilon_{t}^4]-\sigma^4E[\varepsilon_t^4]^2 + E[\varepsilon_t^6] E[\varepsilon_t^4] \sigma^2  -
\sigma^4E[\varepsilon_t^4]^2)\\
&= \frac{E[\varepsilon_t^8]E[\varepsilon_{t}^4] + E[\varepsilon_t^6] E[\varepsilon_t^4] \sigma^2  - 2
\sigma^4E[\varepsilon_t^4]^2}{T}\\
&\to 0
\end{align*} 

(iv) Determine whether the scaled sample means $\sqrt{T}\bar{U}$, $\sqrt{T}\bar{W}$, and $\sqrt{T}\bar{V}$ are asymptotically normal.

$\sqrt{T}\bar{W}$ is not asymptotically normal because $\frac{1}{T}\sum_{t=1}^T W_t^2$ does not converge in probability to its expectation.

We have shown that all but the martingale condition of the martingale central limit theorem hold for $\sqrt{T}\bar{U}$ and $\sqrt{T}\bar{V}$.  For $\sqrt{T}\bar{U}$:

\begin{align*}
E[U_t | U_{t-1}, U_{t-2}, ..., U_1] 
&= E[E[\varepsilon_t \varepsilon_{t-1}|\varepsilon_{t-1}, ..., \varepsilon_0]|U_{t-1}, U_{t-2}, ..., U_1]\\
&= E[\varepsilon_{t-1}E[\varepsilon_t |\varepsilon_{t-1}, ..., \varepsilon_0]|U_{t-1}, U_{t-2}, ..., U_1]\\
&= E[\varepsilon_{t-1}*0|U_{t-1}, U_{t-2}, ..., U_1]\\
&= 0
\end{align*} 

Thus, by the martingale CLT, $\sqrt{T}\bar{U}$ is asymptotically normal. For $\sqrt{T}\bar{V}$:

\begin{align*}
E[V_t | V_{t-1}, V_{t-2}, ..., V_1] 
&= E[E[\varepsilon_t^2 \varepsilon_{t-1}|\varepsilon_{t-1}, ..., \varepsilon_0]|V_{t-1}, V_{t-2}, ..., V_1]\\
&= E[\varepsilon_{t-1}E[\varepsilon_t^2 |\varepsilon_{t-1}, ..., \varepsilon_0]|V_{t-1}, V_{t-2}, ..., V_1]\\
&= E[\varepsilon_{t-1}*\sigma^2|V_{t-1}, V_{t-2}, ..., V_1]\\
&= \sigma^2E[\varepsilon_{t-1}|V_{t-1}, V_{t-2}, ..., V_1]\\
&\neq 0
\end{align*} 

Thus, $\sqrt{T}\bar{V}$ is not asymptotically normal.

\pagebreak

2. Consider a time series of length $T$ from the model 

$$
Y_t = \alpha_0 + t \beta_0 + X_t \delta_0 + Y_{t-1} \rho_1 + U_t
$$

where $Y_0$ and $\{U_t\}_{t=1}^T$ are iid $N(0, 1)$, and

$$
X_t = X_{t-1} \cdot 0.3 + V_t
$$

where $X_0$ and $\{V_t\}_{t=1}^T$ are iid $N(0, 1)$ and independent of $Y_0$ and $\{U_t\}_{t=1}^T$.  We will let $\alpha_0 = \delta_0 = 100$, $\beta_0 = 1$ and consider all combinations of $T \in \{50, 150, 250\}$ and $\rho_1 \in \{0.7, 0.9, 0.95\}$.

(i) In a statistical software of your choice, generate data from (1), estimate the coefficients by OLS, and calculate heteroscedasticity robust two-sided 95% confidence intervals for $\alpha_0$, $\delta_0$, and $\rho_1$.

```{r problem_2i}
tees <- c(50, 150, 250)
rhos <- c(0.7, 0.9, 0.95)
alpha <- 100
delta <- 100
beta <- 1

results <- NULL

for (t in tees) {
  for (rho in rhos) {
    x_t <- rnorm(1)
    y_t <- rnorm(1)
    v_t <- rnorm(t)
    u_t <- rnorm(t)
    
    for (i in 1:t) x_t[i+1] <- 0.3 * x_t[i] + v_t[i]
    for (i in 1:t) y_t[i+1] <- alpha + i * beta + x_t[i+1] * delta + y_t[i] * rho + u_t[i]
    
    x <- cbind(rep(1, t),
               1:t,
               x_t[2:(t+1)],
               y_t[1:t])
    y <- y_t[2:(t+1)]
    
    ols <- solve(t(x) %*% x) %*% (t(x) %*% y)
    
    e_hat <- as.numeric(y - x %*% ols)
    omega <- crossprod(x * e_hat)
    varcov <- solve(t(x) %*% x) %*% omega %*% solve(t(x) %*% x)
    se_robust <- sqrt(diag(varcov))
    
    results <- tibble(t = t,
           rho = rho,
           name = c("alpha", "beta", "delta", "rho"),
           ols = as.numeric(ols),
           se = se_robust) %>% 
      bind_rows(results)
  }
}

results %>%
  mutate(upper_bound = ols + se * 1.96,
         lower_bound = ols - se * 1.96) %>%
  kable(digits = 3)
```

\pagebreak

(ii) Across 10000 simulated repetitions of the above, report the simulated mean of the point estimators for $\alpha_0$, $\delta_0$, and $\rho_1$ and the simulated coverage rate of the confidence intervals.

```{r problem_2ii, eval = FALSE}
ntrials <- 10000
results2 <- NULL

for (t in tees) {
  for (rho in rhos) {
    for (trial in 1:ntrials) {
      print(trial)
      
      x_t <- rnorm(1)
      y_t <- rnorm(1)
      v_t <- rnorm(t)
      u_t <- rnorm(t)
    
      for (i in 1:t) x_t[i+1] <- 0.3 * x_t[i] + v_t[i]
      for (i in 1:t) y_t[i+1] <- alpha + i * beta + x_t[i+1] * delta + 
        y_t[i] * rho + u_t[i]
    
      x <- cbind(rep(1, t),
                 1:t,
                 x_t[2:(t+1)],
                 y_t[1:t])
      y <- y_t[2:(t+1)]
    
      ols <- solve(t(x) %*% x) %*% (t(x) %*% y)
    
      results2 <- tibble(t = t,
                         rho = rho,
                         trial = trial,
                         name = c("alpha", "beta", "delta", "rho"),
                         ols = as.numeric(ols)) %>% 
        bind_rows(results2)
    }
  }
}

save(results2, file = "ps5_vonhafften_temp.RData")
```

\pagebreak

```{r problem_2ii_table}
load("ps5_vonhafften.RData")

results2 %>%
  group_by(t, rho, name) %>%
  summarise(mean = mean(ols),
            lower_bound = quantile(ols, probs = .05),
            upper_bound = quantile(ols, probs = .95),
            .groups = "keep") %>% 
  kable(digits = 3)
```

\pagebreak

(iii) How does sample size and the degree of persistence in $Y_t$ affect the results of the simulations.

```{r problem_2iii, echo=FALSE}
comparison_df <- results %>%
  transmute(t,
            rho,
            name,
            ols,
            upper_bound = ols + se * 1.96,
            lower_bound = ols - se * 1.96,
            method = "Part i") %>%
  bind_rows(results2 %>%
              group_by(t, rho, name) %>%
              summarise(lower_bound = quantile(ols, probs = .05),
                        upper_bound = quantile(ols, probs = .95),
                        ols = mean(ols),
                        method = "Part ii",
                        .groups = "keep"))
```

The three figures below have the point estimate (dots) and confidence intervals (vertical lines) from part i (red) and part ii (blue) where panels differ by sample size (horizontal) and degree of persistence (vertical).  The point estimate for part i is the OLS estimate based on a single trial of simulated data and the confidence interval is the heteroskedastic robust standard error.  The point estimate for part ii is the mean of OLS estimates over 10,000 trials of simulated data and the confidence interval is the 5th and 95th percentile.  Naturally, the point estimates from part ii are closer to the true value than the point estimates from part i. In addition, large sample sizes result in point estimates that are closer to the true value and tighter confidence intervals.  For $\beta$, we see that higher degrees of persistence dramatically expand confidence intervals particularly for small samples.  For $\delta$ and $\alpha$, we that the confidence intervals are similarly sized across degrees of persistence and shrink with larger samples.

```{r problem_2iii_alpha, echo=FALSE}
comparison_df %>% 
  filter(name == "alpha") %>%
  ggplot() +
  geom_point(aes(y=ols, x=method, color=method)) +
  geom_segment(aes(y=lower_bound, yend=upper_bound, x=method, xend=method, color=method)) +
  facet_grid(rho~t) +
  ggtitle("Alpha") +
  theme(legend.position = "none") +
  labs(x="", y="")
```

```{r problem_2iii_beta, echo=FALSE}
comparison_df %>% 
  filter(name == "beta") %>%
  ggplot() +
  geom_point(aes(y=ols, x=method, color=method)) +
  geom_segment(aes(y=lower_bound, yend=upper_bound, x=method, xend=method, color=method)) +
  facet_grid(rho~t) +
  ggtitle("Beta")+
  theme(legend.position = "none")+
  labs(x="", y="")
```

```{r problem_2iii_delta, echo=FALSE}
comparison_df %>% 
  filter(name == "delta") %>%
  ggplot() +
  geom_point(aes(y=ols, x=method, color=method)) +
  geom_segment(aes(y=lower_bound, yend=upper_bound, x=method, xend=method, color=method)) +
  facet_grid(rho~t) +
  ggtitle("Delta")+
  theme(legend.position = "none")+
  labs(x="", y="")

```