---
title: "ECON 709 - PS 6"
author: "Alex von Hafften^[I worked on this problem set with a study group of Michael Nattinger, Andrew Smith, and Ryan Mather. I also discussed problems with Emily Case, Sarah Bass, and Danny Edgel.]"
date: "10/18/2020"
output: pdf_document
header-includes:
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


1. Let $X$ be distributed Bernoulli $P(X = 1) = p$ and $P(X = 0) = 1-p$ for some unknown parameter $0 < p < 1$.

(a) Verify the probability mass function can be written as $f(x) = p^x (1-p)^{(1-x)}$.

$$
f(1) = p^1 (1-p)^{(1-1)}=p=P(X=1)
$$
$$
f(0) = p^0 (1-p)^{(1-0)}=1-p=P(X=0)
$$

(b) Find the log-likelihood function $\ell_n(\theta)$.

\begin{align*}
\ell_n(\theta) 
&= \sum_{i=1}^n\ln(f(x_i|\theta)) \\
&= \sum_{i=1}^n\ln(p^{x_i} (1-p)^{(1-x_i)}) \\
&= \sum_{i=1}^n [x_i \ln(p)+(1-x_i)\ln (1-p)] \\
&= \ln(p) \sum_{i=1}^n x_i + \ln (1-p)\Big(n-\sum_{i=1}^n x_i\Big)
\end{align*}

(c) Find the MLE $\hat{p}$ for $p$.

\begin{align*}
\frac{\partial \ell_n}{\partial p} &= 0 \\
\frac{\partial}{\partial p}\Bigg[\ln(p) \sum_{i=1}^n x_i + \ln (1-p)\Big(n-\sum_{i=1}^n x_i\Big) \Bigg] &= 0\\ \frac{\sum_{i=1}^n x_i}{p} - \frac{\Big(n-\sum_{i=1}^n x_i\Big)}{1-p} &= 0 \\
\sum_{i=1}^n x_i &= pn-p\sum_{i=1}^n x_i + p\sum_{i=1}^n x_i \\
\hat{p} &= \frac{1}{n}\sum_{i=1}^n x_i\\
\end{align*}

\pagebreak

2. Let $X$ be distributed Pareto with density $f(x) = \frac{\alpha}{x^{1+\alpha}}$ for $x \ge 1$. The unknown parameter is $\alpha > 0$.

(a) Find the log-likelihood function $\ell_n(\alpha)$.

\begin{align*}
\ell_n(\alpha) 
&= \sum_{i=1}^n\ln(f(x_i|\alpha)) \\
&= \sum_{i=1}^n\ln\Bigg(\frac{\alpha}{x_i^{1+\alpha}}\Bigg) \\
&= \sum_{i=1}^n\ln\alpha - \sum_{i=1}^n\ln x_i^{1+\alpha} \\
&= n\ln\alpha - (1+\alpha)\sum_{i=1}^n\ln x_i \\
\end{align*}

(b) Find the MLE $\hat{\alpha}_n$ for $\alpha$.

$$
\frac{\partial \ell_n}{\partial \alpha}  =  0 \implies
\frac{n}{\hat{\alpha}_n} - \sum_{i=1}^n\ln x_i = 0 \implies
\hat{\alpha}_n = \frac{n}{\sum_{i=1}^n\ln x_i}
$$

3. Let $X$ be distributed Cauchy with density $f(x) = \frac{1}{\pi(1+(x-\theta)^2)}$ for $x \in \R$. The unknown parameter is $\theta$.

(a) Find the log-likelihood function $\ell_n(\theta)$.

\begin{align*}
\ell_n(\theta) 
&= \sum_{i=1}^n\ln(f(x_i|\theta)) \\
&= \sum_{i=1}^n\ln\Bigg(\frac{1}{\pi(1+(x_i-\theta)^2)}\Bigg) \\
&=  - \sum_{i=1}^n\ln(\pi) - \sum_{i=1}^n\ln\Big(1+(x_i-\theta)^2\Big) \\
&=  - n \ln(\pi) - \sum_{i=1}^n\ln\Big(1+(x_i-\theta)^2\Big) \\
\end{align*}

(b) Find the first-order condition for the MLE $\hat{\theta}$ for $\theta$.  You will not be able to solve for $\hat{\theta}$.

$$
\frac{\partial \ell_n}{\partial \theta}  =  0 \implies 0 - \sum_{i=1}^n\frac{2(x_i-\theta)(-1)}{1+(x_i-\theta)^2} \implies \sum_{i=1}^n\frac{2(x_i-\theta)}{1+(x_i-\theta)^2}
$$

\pagebreak

4. Let $X$ be distributed double exponential (or Laplace) with density $f(x) = \frac{1}{2} \exp(-|x - \theta|)$ for $x \in \R$. The unknown parameter is $\theta$.

(a) Find the log-likelihood function $\ell_n(\theta)$.

\begin{align*}
\ell_n(\theta) 
&= \sum_{i=1}^n\ln(f(x_i|\theta)) \\
&= \sum_{i=1}^n\ln\Bigg(\frac{1}{2} \exp(-|x_i - \theta|)\Bigg) \\
&= -\sum_{i=1}^n\ln(2) + \sum_{i=1}^n\ln(\exp(-|x_i - \theta|)) \\
&= -n\ln(2) - \sum_{i=1}^n|x_i - \theta| \\
\end{align*}

(b) Extra challenge: Find the MLE $\hat{\theta}_n$ for $\theta$. This is challenging as it is not simply solving the FOC due to the nondifferentiability of the density function.

$$
\ell_n(\theta) = -n\ln(2) - \sum_{i=1}^n|x_i - \theta| = -n\ln(2) - \sum_{i=1}^n((x_i - \theta)^2)^{1/2}
$$

$\ell_n(\theta)$ is now differentiable.

$$
\frac{\partial \ell_n}{\partial \theta} = -  (1/2)\sum_{i=1}^n((x_i - \theta)^2)^{-1/2}(2(x_i - \theta))(-1) = \sum_{i=1}^n\frac{x_i - \theta}{|x_i - \theta|}
$$

If $x_i > \theta$, $\frac{x_i - \theta}{|x_i - \theta|} = 1$ and if $x_i < \theta$, $\frac{x_i - \theta}{|x_i - \theta|} = -1$.

...

\pagebreak

5. Take the Pareto model $f(x) = \alpha x^{-1-\alpha}, x \ge 1$. Calculate the information for $\alpha$ using the second derivative.

The information for $\alpha$ is

\begin{align*}
I_0 
&=-E\Bigg[\frac{\partial^2}{\partial^2\alpha} \log (\alpha X^{-1-\alpha}) \Bigg|_{\alpha = \alpha_0}\Bigg]\\
&= -E\Bigg[\frac{\partial^2}{\partial^2\alpha} (-1-\alpha)(\log \alpha +\log X) \Bigg|_{\alpha = \alpha_0}\Bigg]\\
&= -E\Bigg[\frac{\partial^2}{\partial^2\alpha} (-\log \alpha - \log X - \alpha\log \alpha - \alpha\log X) \Bigg|_{\alpha = \alpha_0}\Bigg]\\
&= -E\Bigg[\frac{\partial}{\partial\alpha} \Big(-\frac{1}{\alpha} - \log \alpha -1- \log X\Big) \Bigg|_{\alpha = \alpha_0}\Bigg]\\
&= -E\Bigg[\frac{1}{\alpha^2} - \frac{1}{\alpha} \Bigg|_{\alpha = \alpha_0}\Bigg]\\
&= \frac{1}{\alpha_0} - \frac{1}{\alpha_0^2}
\end{align*}

6. Take the model $f(x) = \theta \exp(-\theta x), x \ge 0, \theta > 0$.

(a) Find the Cramer-Rao lower bound for $\theta$.

\begin{align*}
I_0
&=-E\Bigg[\frac{\partial^2}{\partial^2\theta} \log (\theta \exp(-\theta X)) \Bigg|_{\theta = \theta_0}\Bigg] \\
&=-E\Bigg[\frac{\partial^2}{\partial^2\theta} \log (\theta)-\theta X  \Bigg|_{\theta = \theta_0}\Bigg] \\
&=-E\Bigg[\frac{\partial}{\partial\theta} \frac{1}{\theta}-\theta  \Bigg|_{\theta = \theta_0}\Bigg] \\
&=-E\Bigg[ -\frac{1}{\theta^2}  \Bigg|_{\theta = \theta_0}\Bigg] \\
&=\frac{1}{\theta_0^2}
\end{align*}

Thus, the Cramer-Rao lower bound is $(nI_0)^{-1} = (n\theta_0^{-2})^{-1}= \theta_0^{2}/n$.

\pagebreak

(b) Recall the MLE $\hat{\theta}_n$ for $\theta$ for Problem 1. Notice that this is a function of the sample mean. Use this formula and the delta method to find the asymptotic distribution for $\hat{\theta}_n$.

The log-likelihood function $\ell_n(\theta)$ is

\begin{align*}
\ell_n(\theta) 
&= \sum_{i=1}^n\ln(f(x_i|\theta))\\
&= \sum_{i=1}^n\ln(\theta \exp(-\theta x_i))\\
&= \sum_{i=1}^n(\ln(\theta) -\theta x_i)\\
&= n\ln(\theta) - \theta \sum_{i=1}^n x_i\\
&= n\ln(\theta) - n\theta \bar{X}_n
\end{align*}

Thus, $\hat{\theta}_n$ is

$$
\frac{\partial\ell_n}{\partial \theta} = 0 \implies 0 = \frac{n}{\hat{\theta}_n} - n \bar{X}_n \implies \hat{\theta}_n = \frac{1}{\bar{X}_n}
$$

By the delta method, $\sqrt{n} (\hat{\theta}_n - \theta_0 ) \to_d N(0, V)$ where $V = \Big((-1)\Big(\frac{1}{\theta_0}\Big)^{-2}\Big)^2\sigma^2 = \sigma^2\theta_0^{4}$ and $\sigma^2 = Var(X) = \frac{1}{\theta_0^2}$. Therefore, $\sqrt{n} (\hat{\theta}_n - \theta_0 ) \to_d N(0, \theta_0^2)$

(c) Find the asymptotic distribution for $\hat{\theta}_n$ using the general formula for the asymptotic distribution of MLE introduced in Section 6. Do you find the same answer as in part (b)?

From Section 6, we have that 

$$
\sqrt{n}(\hat{\theta}_n - \theta_0) \to_d N(0, I_0^{-1})
$$

The information of $\theta$ is 

\begin{align*}
I_0 
&= -E \Bigg[\frac{\partial^2}{\partial^2\theta} \log(\theta \exp(-\theta X))\Bigg|_{\theta = \theta_0}\Bigg]\\
&= -E \Bigg[\frac{\partial^2}{\partial^2\theta} \Big( \log(\theta) -\theta X  \Big) \Bigg|_{\theta = \theta_0}\Bigg]\\
&= -E \Bigg[\frac{\partial}{\partial\theta} \Big( \theta^{-1} - X \Big) \Bigg|_{\theta = \theta_0}\Bigg]\\
&= -E \Bigg[-\theta^{-2} \Bigg|_{\theta = \theta_0}\Bigg] \\
&= \theta_0^{-2}
\end{align*}

Therefore, we get the same asymptotic distribution:

$$
\sqrt{n}(\hat{\theta}_n - \theta_0) \to_d N(0, \theta_0^{2})
$$

\pagebreak

7. In the Bernoulli model, you found the asymptotic distribution of the MLE in Problem 2(c).

(a) Propose an estimator of $V$, the asymptotic variance.



(b) Show that this estimator is consistent for $V$ as $n \to \infty$.



(c) Propose a standard error $s(\hat{p_n})$ for the MLE $\hat{p}_n$.\footnote{Recall that the standard error is supposed to approximate the variance of $\hat{p}_n$, not that of the variance of $\sqrt{n}(\hat{p}_n - p$. What would be a reasonable approximation of the variance of $\hat{p}_n$ once you have a reasonable approximation of the variance of $\sqrt{n}(\hat{p}_n - p)$ from part (b)?}



8. Consider the MLE for the upper bound of the uniform distribution in the Uniform Boundary example in Section 3. Assume that $\{X_1, ..., X_n\}$ is a random sample from $Uniform[0, \theta]$. The general asymptotic distribution formula in Section 6 does not apply here because $\ell_n(\theta)$ is not differentiable at the MLE.  But you can derive the asymptotic distribution using the definition of convergences in distribution. Do so by following the steps below.

(a) Let $F_X$ denote the CDF of $Uniform[0, \theta]$. Calculate $F_X(c)$ for all $c \in \R$ based on the PDF of $Uniform[0, \theta]$.



(b) Show that the CDF of $n(\hat{\theta}_n - \theta): F_{n(\hat{\theta}_n - \theta)}(x) = \Pr(\max_{i=1, ..., n}(n(X_i - \theta)) \le x) = (F_X(\theta + \frac{x}{n}))^n$.



(c) Recall that $\lim_{n \to \infty}(1 + \frac{y}{n})^n  = e^y$ for any $y \in \R$. Derive the limit of $F_{n(\hat{\theta}_n - \theta)}(x)$ for all fixed $x \in \R$. (Hint: consider the case where $x < 0$ and the case where $x \ge 0$ separately).



(d) Conclude that $n(\hat{\theta}_n - \theta) \to_d -Z$ for $Z$ being an exponential distribution with parameter $\theta$.



9. Take the model $X \sim N(\mu, \sigma^2)$. Propose a test for $H_0: \mu = 1$ against $H_1: \mu \neq 1$.

Assuming that $\sigma^2$ is unknown, we can use a two-sided t-test by constructing the following t-statistic:

$$
T = \frac{|\sqrt{n}(\bar{X}_n - 1)|}{S_X}
$$

where $S_X^2 = \frac{1}{n-1}(X_i - \bar{X}_n)^2$.  Under the $H_0: \mu = 1$, $T \sim |t_{n-1}|$.  Therefore, $\phi_n(\alpha) = 1(T > t_{\alpha/2, n-1})$ where $t_{\alpha/2, n-1}$ is the $(1-\alpha/2)$ quantile of $t_{n=1}$.

If $\sigma^2$ is known, we can use a z-test by replacing $S_X$ with $\sigma$ in the test statistic:

$$
T = \frac{|\sqrt{n}(\bar{X}_n - 1)|}{\sigma}
$$

Under the $H_0: \mu = 1$, $T \sim |N(0,1)|$. Therefore, $\phi_n(\alpha) = 1(T > z_{\alpha/2})$ where $z_{\alpha/2}$ is the $(1-\alpha/2)$ quantile of a standard normal.

10. Take the model $X \sim N(\mu, 1)$. Consider testing $H_0 : \mu \in \{0, 1\}$ against $H_1 : \mu \notin \{0, 1\}$. Consider the test statistic $T = \min \{|\sqrt{n} \bar{X}_n|, |\sqrt{n} (\bar{X}_n-1)|\}$ Let the critical value be the $1 - \alpha$ quantile of the random variable $\min\{|Z|, |Z - \sqrt{n}|\}$, where $Z \sim N(0, 1)$. Show that $\Pr(T > c | \mu = 1) = \alpha$. Conclude that the size of the test $\phi_n = 1(T > c)$ is $\alpha$.\footnote{Use the fact that $Z$ and $-Z$ have the same distribution. This is an example where the null distribution is the same under different points in a composite null. The test $\phi_n = 1(T > c)$ is called a similar test because $\inf_{\theta_0 \in \Theta_0} \Pr(T > c | \theta = \theta_0) = \sup_{\theta_0 \in \Theta_0} \Pr(T > c | \theta = \theta_0)$.}



$$
\Pr(T > c | \mu = 1) 
= \Pr(\min \{|\sqrt{n} \bar{X}_n|, |\sqrt{n} (\bar{X}_n-1)|\} > c | \mu = 1)
$$

