---
title: "ECON 709 - PS 4"
author: "Alex von Hafften^[I worked on this problem set with a study group of Michael Nattinger, Andrew Smith, and Ryan Mather. I also discussed problems with Emily Case, Sarah Bass, and Danny Edgel.]"
date: "10/4/2020"
output: pdf_document
header-includes:
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Most of the problems assume a random sample $\{X_1, ..., X_n\}$ from a common distribution $F$ with density $f$ such that $E(X) = \mu$ and $Var(X) = \sigma^2$ for generic random variable $X \sim F$. The sample mean and variances are denoted $\bar{X}_n$ and $\hat{\sigma}^2 = n^{-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2$, with the bias corrected variance $s^2 = (n-1)^{-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2$.

1. Suppose that another observation $X_{n+1}$ becomes available.  Show that

(a) $\bar{X}_{n+1} = (n \bar{X}_n + X_{n+1})/(n+1)$

\begin{align*}
(n \bar{X}_n + X_{n+1})/(n+1) &= \Bigg(n n^{-1}\sum_{i=1}^nX_i + X_{n+1}\Bigg)/(n+1) \\
&= \Bigg(\sum_{i=1}^nX_i + X_{n+1}\Bigg)/(n+1) \\
&= \Bigg(\sum_{i=1}^{n+1}X_i\Bigg)/(n+1) \\
&= \bar{X}_{n+1}
\end{align*}

\pagebreak

(b) $s^2_{n+1}= ((n-1)s_n^2 + (n/(n+1))(X_{n+1}-\bar{X}_n)^2)/n$

\begin{align*}
s^2_{n+1} &= n^{-1} \sum_{i=1}^{n+1} (X_i - \bar{X}_{n+1})^2 \\
&= n^{-1} \sum_{i=1}^{n+1} [(X_i - \bar{X}_n) + (\bar{X}_n - \bar{X}_{n+1})]^2 \\
&= n^{-1} \sum_{i=1}^{n+1} \Bigg[(X_i - \bar{X}_n) + \Bigg(\bar{X}_n - \frac{n \bar{X}_n + X_{n+1}}{n+1}\Bigg)\Bigg]^2 \\
&= n^{-1} \sum_{i=1}^{n+1} \Bigg[(X_i - \bar{X}_n) + \Bigg(\frac{\bar{X}_n - X_{n+1}}{n+1}\Bigg)\Bigg]^2 \\
&= n^{-1} \sum_{i=1}^{n+1} \Bigg[(X_i - \bar{X}_n)^2 + 2(X_i - \bar{X}_n)\Bigg(\frac{\bar{X}_n - X_{n+1}}{n+1}\Bigg) + \Bigg(\frac{\bar{X}_n - X_{n+1}}{n+1}\Bigg)^2 \Bigg] \\
&= n^{-1} \Bigg[ \sum_{i=1}^{n+1} (X_i - \bar{X}_n)^2 + 2\Bigg(\frac{\bar{X}_n - X_{n+1}}{n+1}\Bigg)\sum_{i=1}^{n+1}(X_i - \bar{X}_n) + (n+1)\Bigg(\frac{\bar{X}_n - X_{n+1}}{n+1}\Bigg)^2 \Bigg] \\
&= n^{-1} \Bigg[ \sum_{i=1}^{n} (X_i - \bar{X}_n)^2 + (X_{n+1} - \bar{X}_n)^2 + 2\Bigg(\frac{\bar{X}_n - X_{n+1}}{n+1}\Bigg)\Bigg(\sum_{i=1}^{n}X_i +X_{n+1} - (n+1)\bar{X}_n\Bigg) + \frac{(\bar{X}_n - X_{n+1})^2}{n+1} \Bigg] \\
&= n^{-1} \Bigg[ (n-1)s^2_n + (X_{n+1} - \bar{X}_n)^2 + 2\Bigg(\frac{\bar{X}_n - X_{n+1}}{n+1}\Bigg)\Bigg(X_{n+1} - \bar{X}_n\Bigg) + \frac{(\bar{X}_n - X_{n+1})^2}{n+1} \Bigg] \\
&= n^{-1} \Bigg[ (n-1)s^2_n + (X_{n+1} - \bar{X}_n)^2 - 2\Bigg(\frac{(\bar{X}_n - X_{n+1})^2}{n+1}\Bigg) + \frac{(\bar{X}_n - X_{n+1})^2}{n+1} \Bigg] \\
&= n^{-1} \Bigg[ (n-1)s^2_n + (X_{n+1} - \bar{X}_n)^2 - \frac{(\bar{X}_n - X_{n+1})^2}{n+1} \Bigg] \\
&= n^{-1} \Bigg[ (n-1)s_n^2 + \frac{n}{n+1}(X_{n+1}-\bar{X}_n)^2 \Bigg]
\end{align*}

\pagebreak

2. For some integer $k$, set $\mu_k=E(X^k)$. Construct an unbiased estimator $\hat{\mu}_k$ for $\mu_k$, and show its unbiasedness.

Consider sample raw moments: $\hat{\mu}_k = \frac{1}{n} \sum_{i=1}^n X_i^k$. Raw sample moments are unbiased:

$$
E(\hat{\mu}_k) = E\Bigg( \frac{1}{n} \sum_{i=1}^n X_i^k \Bigg)= \frac{1}{n}\sum_{i=1}^n E(X_i^k)=\frac{1}{n}\sum_{i=1}^n \mu_k=\mu_k
$$

3. Consider the central moment $m_k = E((X- \mu)^k)$. Construct an estimator $\hat{m}_k$ for $m_k$ without assuming a known $\mu$. In general, do you expect $\hat{m}_k$ to be biased or unbiased?

Consider sample central moments: $\hat{m}_k = \frac{1}{n} \sum_{i=1}^n (X_i-\bar{X}_n)^k$. In general, I expect $\hat{m}_k$ to be biased.  For example, as shown in lecture, $\hat{m}_2 = \hat{\sigma}_2$ is a biased estimator for variance $\sigma_2$.

4. Calculate the variance of $\hat{\mu}_k$ that you proposed above, and call it $Var(\hat{\mu_k})$.

\begin{align*}
Var(\hat{\mu}_k) 
&= E(\hat{\mu}_k^2) - E(\hat{\mu}_k)^2\\
&= E\Bigg( \Bigg(\frac{1}{n} \sum_{i=1}^n X_i^k \Bigg)^2\Bigg)-\mu_k^2 \\
&= \frac{1}{n^2}E\Bigg( \Bigg( \sum_{i=1}^n X_i^k \Bigg)^2\Bigg)-\mu_k^2 \\
&= \frac{1}{n^2}E\Bigg( \sum_{i=1}^n\sum_{j=1}^n X_i^kX_j^k \Bigg)-\mu_k^2 \\
&= \frac{1}{n^2}E\Bigg( \sum_{i=1}^n X_i^{2k} + \sum_{i=1}^n\sum_{j=1; i \neq j}^n X_i^kX_j^k \Bigg)-\mu_k^2 \\
&= \frac{1}{n^2} \sum_{i=1}^n E[X_i^{2k}] + \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1; i \neq j}^n E[X_i^k]E[X_j^k] -\mu_k^2 \\
&= \frac{1}{n^2} \sum_{i=1}^n \mu_{2k} + \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1; i \neq j}^n \mu_k^2 -\mu_k^2 \\
&= \frac{1}{n^2} n\mu_{2k} + \frac{1}{n^2}(n^2-n) \mu_k^2 -\mu_k^2 \\
&= \frac{1}{n} \mu_{2k} + \mu_k^2-\frac{\mu_k^2}{n}  -\mu_k^2 \\
&= \frac{\mu_{2k} - \mu_k^2}{n} 
\end{align*}

\pagebreak

5. Show that $E(s_n) \le \sigma$. (Hint: Use Jensen's inequality, CB Theorem 4.7.7).

Because $g(x) = \sqrt{x}$ is a concave function, we can apply Jensen's inequality:

$$
E(s_n) = E\Bigg(\sqrt{(n-1)^{-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2}\Bigg) \le \sqrt{E\Bigg((n-1)^{-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2\Bigg)} = \sqrt{\sigma^2}=\sigma
$$

6. Show algebraically that $\hat{\sigma}^2=n^{-1}\sum_{i=1}^n (X_i - \mu)^2-(\bar{X}_n-\mu)^2$.

I show that $\hat{\sigma}^2=n^{-1} \Bigg(\sum_{i=1}^n X_i^2  -n\bar{X}_n^2 \Bigg)$ and $n^{-1}\sum_{i=1}^n (X_i - \mu)^2-(\bar{X}_n-\mu)^2=n^{-1} \Bigg(\sum_{i=1}^n X_i^2  -n\bar{X}_n^2 \Bigg)$, so by transitivity $\hat{\sigma}^2=n^{-1}\sum_{i=1}^n (X_i - \mu)^2-(\bar{X}_n-\mu)^2$:

\begin{align*}
n^{-1}\sum_{i=1}^n (X_i - \mu)^2-(\bar{X}_n-\mu)^2 
&= n^{-1}\sum_{i=1}^n (X_i^2 -2X_i\mu + \mu^2)-(\bar{X}_n^2-2\bar{X}_n\mu + \mu^2)\\
&= n^{-1} \Bigg(\sum_{i=1}^n X_i^2 -2\mu\sum_{i=1}^nX_i + n\mu^2-n\bar{X}_n^2+2n\bar{X}_n\mu -n\mu^2\Bigg)\\
&= n^{-1} \Bigg(\sum_{i=1}^n X_i^2  -n\bar{X}_n^2 \Bigg)\\
\end{align*}

\begin{align*}
\hat{\sigma}^2 
&= n^{-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2 \\
&= n^{-1} \sum_{i=1}^n (X_i^2 -2X_i\bar{X}_n+ \bar{X}_n^2) \\
&= n^{-1} \Bigg(\sum_{i=1}^n X_i^2 - 2\bar{X}_n \sum_{i=1}^n X_i+ \sum_{i=1}^n\bar{X}_n^2\Bigg) \\
&= n^{-1} \Bigg(\sum_{i=1}^n X_i^2 - 2\bar{X}_n n\bar{X}_n+ n\bar{X}_n^2\Bigg) \\
&= n^{-1} \Bigg(\sum_{i=1}^n X_i^2 - n\bar{X}_n^2\Bigg) \\
\end{align*}

\pagebreak

7. Find the covariance of $\hat{\sigma}^2$ and $\bar{X}_n$. Under what condition is this zero?

$$
E\Big[(\bar{X}_n-E(\bar{X}_n))(\hat{\sigma}^2 - E(\hat{\sigma}^2))\Big] = 
E\Big[(\bar{X}_n-\mu)(\hat{\sigma}^2 - \sigma^2)\Big]
$$

\pagebreak

8. Suppose that $X_i$ are i.n.i.d (independent but not necessarily identically distributed) with $E(X_i)=\mu_i$ and $Var(X_i)=\sigma_i^2$.

(a) Find $E(\bar{X}_n)$.

$$
E(\bar{X}_n) = E\Bigg(n^{-1} \sum_{i=1}^nX_i\Bigg) = n^{-1} \sum_{i=1}^nE(X_i) = n^{-1} \sum_{i=1}^n\mu_i
$$

(b) Find $Var(\bar{X}_n)$.


$$
Var(\bar{X}_n) = Var\Bigg(n^{-1}\sum_{i=1}^n X_i\Bigg) = n^{-2} \sum_{i=1}^n Var(X_i) = n^{-2} \sum_{i=1}^n \sigma_i^2
$$

\pagebreak

9. Show that if $Q \sim \chi^2_r$, then $E(Q) = r$ and $Var(Q)=2r$. 

Note that $Q = \sum_{i=1}^n X_i^2$ with $X_i \sim N(0,1)$), then $M_X(t) = \exp\Big(\frac{1}{2}t^2\Big)$.

\begin{align*}
M_X^{(1)}(t) 
&= \exp\Big(\frac{1}{2}t^2\Big)t\\
M_X^{(2)}(t) 
&= \exp\Big(\frac{1}{2}t^2\Big) + \exp\Big(\frac{1}{2}t^2\Big)t^2\\
M_X^{(3)}(t) 
&= \exp\Big(\frac{1}{2}t^2\Big)t + \exp\Big(\frac{1}{2}t^2\Big)t^3 + 2\exp\Big(\frac{1}{2}t^2\Big)t \\
&= 3\exp\Big(\frac{1}{2}t^2\Big)t + \exp\Big(\frac{1}{2}t^2\Big)t^3 \\
M_X^{(4)}(t) 
&= 3\exp\Big(\frac{1}{2}t^2\Big) + 3\exp\Big(\frac{1}{2}t^2\Big)t^2 + \exp\Big(\frac{1}{2}t^2\Big)t^4 + 3\exp\Big(\frac{1}{2}t^2\Big)t^2\\
&= \exp\Big(\frac{1}{2}t^2\Big)t^4 + 6\exp\Big(\frac{1}{2}t^2\Big)t^2 + 3\exp\Big(\frac{1}{2}t^2\Big)
\end{align*}

\begin{align*}
E[X] &= M_X^{(1)}(0)= 0 \\
E[X^2] &= M_X^{(2)}(0)= 1 \\
E[X^3] &= M_X^{(3)}(0)= 0 \\
E[X^4] &= M_X^{(4)}(0)= 3\\
\end{align*}

\begin{align*}
E(Q) = E\Bigg( \sum_{i=1}^r X_i^2 \Bigg) = \sum_{i=1}^r E(X_i^2) = \sum_{i=1}^r (1)= r
\end{align*}

\begin{align*}
Var(Q) &= E(Q^2)-E(Q)^2 \\
&= E\Bigg(\Bigg(\sum_{i=1}^r X_i^2\Bigg)^2\Bigg)-r^2 \\
&= E\Bigg(\sum_{i=1}^r\sum_{j=1}^r X_i^2X_j^2 \Bigg) - r^2\\
&= E\Bigg(\sum_{i=1}^rX_i^4 + \sum_{i=1}^r\sum_{j=1; j \neq i}^r X_i^2X_j^2\Bigg)-r^2\\
&= \sum_{i=1}^rE(X_i^4) + \sum_{i=1}^r\sum_{j=1; j \neq i}^r E(X_i^2)E(X_j^2)-r^2 \\
&= \sum_{i=1}^r (3) + \sum_{i=1}^r\sum_{j=1; j \neq i}^r (1)(1)-r^2 \\
&= 3r + r(r-1)-r^2 \\
&= 2r
\end{align*}

\pagebreak

10. Suppose that $X_i \sim N(\mu_X, \sigma_X^2): i = 1,...,n_1$ and $Y_i \sim N(\mu_Y, \sigma_Y^2): i = 1,...,n_2$ are mutually independent. Set $\bar{X}_n = n_1^{-1} \sum_{i=1}^{n_1} X_i$ and $\bar{Y}_n=n_2^{-1}\sum_{i=1}^{n_2}Y_i$.

(a) Find $E(\bar{X}_n -\bar{Y}_n)$.

\begin{align*}
E(\bar{X}_n -\bar{Y}_n) 
&= E\Bigg(n_1^{-1} \sum_{i=1}^{n_1} X_i - n_2^{-1}\sum_{i=1}^{n_2}Y_i \Bigg) \\
&= n_1^{-1} \sum_{i=1}^{n_1} E(X_i) - n_2^{-1}\sum_{i=1}^{n_2} E(Y_i) \\
&= n_1^{-1} \sum_{i=1}^{n_1} \mu_X - n_2^{-1}\sum_{i=1}^{n_2} \mu_Y \\
&= \mu_X - \mu_Y
\end{align*}

(b) Find $Var(\bar{X}_n -\bar{Y}_n)$.

\begin{align*}
Var(\bar{X}_n -\bar{Y}_n) 
&= Var\Bigg(n_1^{-1} \sum_{i=1}^{n_1} X_i + n_2^{-1}\sum_{i=1}^{n_2}Y_i \Bigg) \\
&= n_1^{-2} \sum_{i=1}^{n_1} Var(X_i) + n_2^{-2}\sum_{i=1}^{n_2} Var(Y_i) \\
&= n_1^{-2} \sum_{i=1}^{n_1} \sigma_X^2 + n_2^{-2}\sum_{i=1}^{n_2} \sigma_Y^2  \\
&= \frac{\sigma_X^2}{n_1} + \frac{\sigma_Y^2}{n_2} \\
\end{align*}

\pagebreak

(c) Find the distribution of $\bar{X}_n - \bar{Y}_n$.

Consider any independent $W \sim N(\mu_W, \sigma_W)$ and $Z \sim N(\mu_Z, \sigma_Z)$. Therefore:

\begin{align*}
M_W(t) 
&= \exp\Bigg(t\mu_W + \frac{t^2}{2}\sigma_W^2\Bigg) \\
M_Z(t) 
&= \exp\Bigg(t\mu_Z + \frac{t^2}{2}\sigma_Z^2\Bigg) \\
M_{W+Z}(t) 
&= E[\exp(t(W+Z))] \\
&= E[\exp(tW)\exp(tZ)]\\
&= E[\exp(tW)]E[\exp(tZ)] \\
&= M_W(t)M_Z(t) \\
&= \exp\Bigg(t\mu_W + \frac{t^2}{2}\sigma_W^2\Bigg)\exp\Bigg(t\mu_Z + \frac{t^2}{2}\sigma_Z^2\Bigg) \\
&= \exp\Bigg(t\mu_W + \frac{t^2}{2}\sigma_W^2+t\mu_Z + \frac{t^2}{2}\sigma_Z^2\Bigg)\\
&= \exp\Bigg(t(\mu_W+\mu_Z) + \frac{t^2}{2}(\sigma_W^2+\sigma_Z^2)\Bigg)
\end{align*}

So $W+Z \sim N(\mu_W+\mu_Z, \sigma_W^2+\sigma_Z^2)$.

By induction, $\sum_{i=1}^{n_1} X_i \sim(n_1\mu_X, n_1\sigma_X^2)$ and $\sum_{i=1}^{n_2} Y_i \sim(n_2\mu_Y, n_2\sigma_Y^2)$. So $\bar{X}_{n} \sim N(\mu_X, n_1^{-1}\sigma_X^2)$ and $\bar{Y}_{n} \sim N (\mu_Y, n_2^{-1}\sigma_Y^2)$.  And $\bar{X}_{n} - \bar{Y}_{n} \sim N\Bigg(\mu_X - \mu_Y, \frac{\sigma_X^2}{n_1} + \frac{\sigma_Y^2}{n_2}\Bigg)$.