---
title: "ECON 709 - PS 3"
author: "Alex von Hafften^[I worked on this problem set with a study group of Michael Nattinger, Andrew Smith, and Ryan Mather. I also discussed problems with Emily Case, Sarah Bass, and Danny Edgel.]"
date: "9/27/2020"
output: pdf_document
header-includes:
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. A random point $(X, Y)$ is distributed uniformly on the square with vertices $(1, 1), (1, -1), (-1, 1)$, and $(-1, -1)$. That is, the joint PDF is $f(x, y) = 1/4$ on the square and $f(x, y) = 0$ outside the square. Determine the probability of the following events:

(a) $X^2 + Y^2 < 1$

Case 1: $X=1$ or $X=-1$ (left and right edge of the box). $X^2 + Y^2 < 1 \implies 1+Y^2<1$.  Since $Y^2 \ge 0 \implies P(1+Y^2<1)=0$.

Case 2: $Y=1$ or $Y=-1$ (top and bottom edge of the box). $X^2 + Y^2 < 1 \implies X^2+1<1$.  Since $X^2 \ge 0 \implies P(X^2+1<1)=0$. 

Therefore, $P(X^2 + Y^2 < 1) = 0$.

(b) $|X + Y| < 2$

Notice that only two points on the box do not meet $|X + Y| < 2$.  At $(1, 1)$ and $(-1, -1)$, $|1 + 1|=|-1+(-1)|=2$.  At all other points, $|X + Y| \in [0, 2)$.  Since $X$ and $Y$ are continuous random variables, the probability that they equal a given point is zero, so $P(|X + Y| < 2)=1$.

2. Let the joint PDF of $X$ and $Y$ be given by $f(x, y) = g(x)h(y)$ $\forall x, y \in \R$ for some functions $g(x)$ and $h(y)$. Let $a$ denote $\int_{-\infty}^\infty g(x)dx$ and $b$ denote $\int_{-\infty}^\infty h(x)dx$

(a) What conditions $a$ and $b$ should satisfy in order for $f(x, y)$ to be a bivariate PDF?

For $f(x, y)$ to be a PDF, it should integrate to one:

\begin{align*}
\int_{-\infty}^\infty \int_{-\infty}^\infty f(x, y) dx dy &= 1 \\
\implies \int_{-\infty}^\infty \int_{-\infty}^\infty g(x)h(y) dx dy &= 1 \\
\implies \int_{-\infty}^\infty g(x) dx \int_{-\infty}^\infty h(y) dy &= 1 \\
\implies a b &= 1 \\
\implies a &= b^{-1} \\
\end{align*}

\pagebreak

(b) Find the marginal PDF of $X$ and $Y$.

The marginal PDF of $X$:

\begin{align*}
f_X(x) &= \int_{-\infty}^\infty f(x, y) dy \\
       &= \int_{-\infty}^\infty g(x)h(y) dy \\
       &= g(x)\int_{-\infty}^\infty h(y) dy \\
       &= b \cdot g(x) \\
\end{align*}

The marginal PDF of $Y$:

\begin{align*}
f_Y(y) &= \int_{-\infty}^\infty f(x, y) dx \\
       &= \int_{-\infty}^\infty g(x)h(y) dx \\
       &= h(y)\int_{-\infty}^\infty g(x) dx \\
       &= a \cdot h(y) \\
\end{align*}

(c) Show that $X$ and $Y$ are independent.

Proof: $X$ and $Y$ are independent if the product of their marginal distributions is their joint distribution:

\begin{align*}
f_X(x) \cdot f_Y(y) &= b \cdot g(x) \cdot a \cdot h(y) \\
                    &= b \cdot g(x) \cdot b^{-1} \cdot h(y) \\
                    &= g(x) \cdot h(y) \\
                    &= f(x, y) \\
\end{align*}

$\square$

\pagebreak

3. Let the joint PDF of $X$ and $Y$ be given by 

\begin{align*}
f(x, y) = \begin{cases} cxy & \text{ if } x, y \in [0, 1], x+y \le 1 \\ 0, & \text{ otherwise } \end{cases}
\end{align*}

(a) Find the value of $c$ such that $f(x, y)$ is a joint PDF.

\begin{align*}
        \int_0^1 \int_0^{1-x} f(x, y) dy dx &= 1 \\
\implies    \int_0^1 \int_0^{1-x} cxy dy dx &= 1 \\
\implies c\int_0^1 \Big[ \frac{xy^2}{2} \Big]_{y=0}^{1-x} dx &= 1\\
\implies \frac{c}{2} \int_0^1 x(1-x)^2 dx &= 1\\
\implies \frac{c}{2} \int_0^1 x-2x^2+x^3 dx &= 1\\
\implies \frac{c}{2}\Big[ \frac{x^2}{2}-\frac{2x^3}{3}+\frac{x^4}{4} \Big]_{x=0}^1 &= 1\\
\implies \frac{c}{2}\Big[ \frac{1}{2}-\frac{2}{3}+\frac{1}{4} \Big] &= 1\\
\implies \frac{c}{2}\Big(\frac{1}{12}\Big) &= 1\\
\implies c &= 24\\
\end{align*}

(b) Find the marginal distributions of $X$ and $Y$.

\begin{align*}
f_X(x) = \int_0^{1-x} 24xy dy
\end{align*}

\begin{align*}
f_Y(y) = \int_0^{1-y} 24xy dx
\end{align*}

(c) Are $X$ and $Y$ independent? Compare your answer to Problem 2 and discuss.

4. Show that any random variable is uncorrelated with a constant.

5. Let $X$ and $Y$ be independent random variables with means $\mu_X, \mu_Y$ and variances $\sigma_X^2, \sigma_Y^2$.  Find an expression for the correlation of $XY$ and $Y$ in terms of these means and variances.

6. Prove the following: For any random vector $(X_1, X_2, ..., X_n)$,

$$
Var\Bigg( \sum_{i=1}^n X_i\Bigg) = \sum_{i=1}^n Var (X_i) + 2 \sum_{1\le i < j \le n}
Cov(X_i, Y_i).
$$

7. Suppose that $X$ and $Y$ are joint normal, i.e. they have the joint PDF:

$$
f(x, y) = \frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1-\rho^2}}\exp(-(2(1-\rho^2))^{-1}(x^2/\sigma_X-2xy/\sigma_X\sigma_Y+y^2/\sigma_Y^2))
$$

(a) Derive the marginal distributions of $X$ and $Y$, and observe that both normal distributions.

(b) Derive the conditional distribution of $Y$ given $X=x$. Observe that it is also a nromal distribution.

(c) Derive the joint distribution of $(X, Z)$ where $Z = (Y/\sigma_Y)-(\rho X/\sigma_X)$, and then show that $X$ and $Z$ are independent.

8. Consider a function $g : \R \to \R$. Recall that the inverse image of a set $A$, denoted $g^{-1}(A)$ is $g^{-1}(A)=\{x \in \R: g(x) \in A\}$.  Let there be functions $g_1: \R \to \R$ and $g_2: \R \to \R$.  Let $X$ and $Y$ be two random variables that are independent.  Suppose that $g_1$ and $g_2$ are both Borel-measurable, which means that $g_1^{-1}(A)$ and $g_2^{-1}(A)$ are both in the Borel $\sigma$-field whenever $A$ is in the Borel $\sigma$-field.  Show that the two random variables $Z := g_1(X)$ and $W:=g_2(Y)$ are independent.  (Hint: use the 1st or the 2nd definition of independence.)


