---
title: "ECON 709 - PS 3"
author: "Alex von Hafften^[I worked on this problem set with a study group of Michael Nattinger, Andrew Smith, and Ryan Mather. I also discussed problems with Emily Case, Sarah Bass, and Danny Edgel.]"
date: "9/27/2020"
output: pdf_document
header-includes:
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. A random point $(X, Y)$ is distributed uniformly on the square with vertices $(1, 1), (1, -1), (-1, 1)$, and $(-1, -1)$. That is, the joint PDF is $f(x, y) = 1/4$ on the square and $f(x, y) = 0$ outside the square. Determine the probability of the following events:

(a) $X^2 + Y^2 < 1$

$X^2 + Y^2 < 1 \implies -\sqrt{1 - X^2} < Y < \sqrt{1 - X^2}$

$$ \int_{-1}^1 \int_{-\sqrt{1 - x^2}}^{\sqrt{1 - x^2}} \frac{1}{4}dydx =  \int_{-1}^1 \Bigg[\frac{1}{4}y\Bigg]_{-\sqrt{1 - x^2}}^{\sqrt{1 - x^2}}dx =  \frac{1}{2}\int_{-1}^1 \sqrt{1 - x^2}dx$$

Define $x = \sin \theta \implies dx = \cos \theta d \theta$.

\begin{align*}
\frac{1}{2}\int_{-1}^1 \sqrt{1 - x^2}dx &= \frac{1}{2}\int_{-\pi/2}^{\pi/2} \sqrt{1 - (\sin \theta)^2} \cos \theta d\theta \\
                                        &= \frac{1}{2}\int_{-\pi/2}^{\pi/2} \cos^2 \theta d\theta \\
                                        &= \frac{1}{4}\int_{-\pi/2}^{\pi/2} 1+\cos (2\theta) d\theta \\
                                        &= \frac{1}{4} \Bigg[\theta+\frac{\sin (2\theta)}{2}\Bigg]_{-\pi/2}^{\pi/2} \\
                                        &= \frac{1}{4} \Bigg[(\pi/2)+\frac{0}{2} - (-\pi/2) - \frac{0}{2}\Bigg] \\
                                        &= \frac{\pi}{4}
\end{align*}

(b) $|X + Y| < 2$

$|X + Y| < 2 \implies -2 < X +Y < 2 \implies -2-X<Y<2-X$. Since $X$ ranges from -1 to 1, $-2-X<Y<2-X \implies -1<Y<1$

$$ \int_{-1}^1 \int_{-1}^{1} \frac{1}{4}dydx = \frac{1}{4} \int_{-1}^1 [ y ]_{-1}^{1} dx  = \frac{1}{2} \int_{-1}^1 dx = \frac{1}{2} [x]_{-1}^1 = 1$$

\pagebreak

2. Let the joint PDF of $X$ and $Y$ be given by $f(x, y) = g(x)h(y)$ $\forall x, y \in \R$ for some functions $g(x)$ and $h(y)$. Let $a$ denote $\int_{-\infty}^\infty g(x)dx$ and $b$ denote $\int_{-\infty}^\infty h(x)dx$

(a) What conditions $a$ and $b$ should satisfy in order for $f(x, y)$ to be a bivariate PDF?

For $f(x, y)$ to be a PDF, it should integrate to one:

\begin{align*}
\int_{-\infty}^\infty \int_{-\infty}^\infty f(x, y) dx dy &= 1 \\
\implies \int_{-\infty}^\infty \int_{-\infty}^\infty g(x)h(y) dx dy &= 1 \\
\implies \int_{-\infty}^\infty g(x) dx \int_{-\infty}^\infty h(y) dy &= 1 \\
\implies a b &= 1 \\
\implies a &= b^{-1} \\
\end{align*}

(b) Find the marginal PDF of $X$ and $Y$.

The marginal PDF of $X$:

$$
f_X(x) = \int_{-\infty}^\infty f(x, y) dy = \int_{-\infty}^\infty g(x)h(y) dy = g(x)\int_{-\infty}^\infty h(y) dy = b \cdot g(x)
$$

The marginal PDF of $Y$:

$$
f_Y(y) = \int_{-\infty}^\infty f(x, y) dx = \int_{-\infty}^\infty g(x)h(y) dx = h(y)\int_{-\infty}^\infty g(x) dx = a \cdot h(y)
$$

(c) Show that $X$ and $Y$ are independent.

Proof: $X$ and $Y$ are independent if the product of their marginal distributions is their joint distribution:

\begin{align*}
f_X(x) \cdot f_Y(y) &= b \cdot g(x) \cdot a \cdot h(y) \\
                    &= b \cdot g(x) \cdot b^{-1} \cdot h(y) \\
                    &= g(x) \cdot h(y) \\
                    &= f(x, y) \\
\end{align*}

$\square$

\pagebreak

3. Let the joint PDF of $X$ and $Y$ be given by 

\begin{align*}
f(x, y) = \begin{cases} cxy & \text{ if } x, y \in [0, 1], x+y \le 1 \\ 0, & \text{ otherwise } \end{cases}
\end{align*}

(a) Find the value of $c$ such that $f(x, y)$ is a joint PDF.

\begin{align*}
        \int_0^1 \int_0^{1-x} f(x, y) dy dx &= 1 \\
\implies    \int_0^1 \int_0^{1-x} cxy dy dx &= 1 \\
\implies c\int_0^1 \Big[ \frac{xy^2}{2} \Big]_{y=0}^{1-x} dx &= 1\\
\implies \frac{c}{2} \int_0^1 x(1-x)^2 dx &= 1\\
\implies \frac{c}{2}\Big[ \frac{x^2}{2}-\frac{2x^3}{3}+\frac{x^4}{4} \Big]_{x=0}^1 &= 1\\
\implies \frac{c}{2}\Big[ \frac{1}{2}-\frac{2}{3}+\frac{1}{4} \Big] &= 1\\
\implies c &= 24\\
\end{align*}

(b) Find the marginal distributions of $X$ and $Y$.

$$
f_X(x) = \int_0^{1-x} f(x,y) dy = \int_0^{1-x} 24xy dy = \Big[ 12xy^2 \Big]_{y=0}^{1-x} = \begin{cases} 12x(1-x)^2, x \in [0, 1] \\ 0, \text{ otherwise.} \end{cases}
$$

$$
f_Y(y) = \int_0^{1-y} f(x,y) dx = \int_0^{1-y} 24xy dx = \Big[ 12x^2y \Big]_{x=0}^{1-y} = \begin{cases} 12(1-y)^2 y, y \in [0, 1] \\ 0, \text{ otherwise.} \end{cases}
$$

(c) Are $X$ and $Y$ independent? Compare your answer to Problem 2 and discuss.

$X$ and $Y$ independent if the product of the marginal distributions equals their joint distribution at all points in the support.  If $x=y=0.9$, $f(0.9, 0.9) = 0$ because $(0.9, 0.9)$ is not in the support, $x+y=0.9+0.9=1.8 > 1$. But each marginal distribution is define over $[0,1]$, so the product of the marginals is positive at $(0.9, 0.9)$: $f_X(0.9)f_Y(0.9) = [12(0.9)(1-(0.9))^2][12(1-0.9)^2 (0.9)] = 0.0117$.

In (2), the support for the joint distribution is $\R^2$, whereas the support for the joint distribution depends on the realization of the random variable.

\pagebreak

4. Show that any random variable is uncorrelated with a constant.

Proof: Let $a \in \R$ and $X$ be a random variable with distribution $F_X$. Define random variable $Y$ as the degenerate random variable that equals $a$.  Thus, the distribution $Y$ is

\begin{align*}
F_Y(y) = \begin{cases} 0, & y<a \\ 1, & y \ge a \end{cases}
\end{align*}

To show $X$ is uncorrelated with a constant, I show that $X$ and $Y$ are independent and then, by a theorem in the Lecture 3 Notes, we know that $X$ and $Y$ are uncorrelated.

To find the joint distribution of $X$ and $Y$, consider two cases: $y < a$ and $y \ge a$. For $y < a$,

\begin{align*}
F(x, y) &= P(X \le x \text{ and } Y \le y)\\
        &= P(X \le x \text{ and } Y \le a) \\
        &= 0
\end{align*}

For $y \ge a$:

\begin{align*}
F(x, y) &= P(X \le x \text{ and } Y \le y)\\
        &= P(X \le x) \\
        &= F_X(x)
\end{align*}

Thus, the joint distribution is

\begin{align*}
F(x, y) = \begin{cases} 0, & y<a \\ F_X(x), & y \ge a \end{cases}
\end{align*}

The joint distribution equals the product of the marginals:

\begin{align*}
F(x, y) = \begin{cases} 0*F_X(x), & y<a \\ 1*F_X(x), & y \ge a \end{cases} = \begin{cases} F_Y(y)*F_X(x), & y<a \\ F_Y(y)*F_X(x), & y \ge a \end{cases}.
\end{align*}

$\square$

\pagebreak

5. Let $X$ and $Y$ be independent random variables with means $\mu_X, \mu_Y$ and variances $\sigma_X^2, \sigma_Y^2$.  Find an expression for the correlation of $XY$ and $Y$ in terms of these means and variances.

Note that $Var(X) = E(X^2) - E(X)^2 \implies E(X^2) = Var(X) + E(X)^2$.

\begin{align*}
Corr(XY, Y) 
&= \frac{Cov(XY, Y)}{\sqrt{Var(XY) Var(Y)}} \\
&= \frac{E(XY^2)-E(XY)E(Y)}{\sigma_Y\sqrt{E((XY)^2)-E(XY)^2}} \\
&= \frac{E(X)E(Y^2)-E(X)E(Y)E(Y)}{\sigma_Y\sqrt{E(X^2)E(Y^2)-(E(X)E(Y))^2}} \\
&= \frac{\mu_X(Var(Y) + E(Y)^2)-\mu_X\mu_Y^2}{\sigma_Y\sqrt{(Var(X) + E(X)^2)(Var(Y) + E(Y)^2)-(\mu_X\mu_Y)^2}} \\
&= \frac{\mu_X\sigma_Y^2 + \mu_X\mu_Y^2-\mu_X\mu_Y^2}{\sigma_Y\sqrt{\sigma_Y^2\sigma_X^2 + \mu_X^2\sigma_Y^2 + \mu_Y^2\sigma_X^2 + \mu_Y^2\mu_X^2-\mu_X^2\mu_Y^2}} \\
&= \frac{\mu_X\sigma_Y}{\sqrt{\sigma_Y^2\sigma_X^2 + \mu_X^2\sigma_Y^2 + \mu_Y^2\sigma_X^2}}
\end{align*}

\pagebreak

6. Prove the following: For any random vector $(X_1, X_2, ..., X_n)$,

$$
Var\Bigg( \sum_{i=1}^n X_i\Bigg) = \sum_{i=1}^n Var (X_i) + 2 \sum_{1\le i < j \le n}Cov(X_i, X_j).
$$

Proof (by induction): For $n = 2$, $Var(X_1+X_2) = Var (X_1) + Var (X_2) + 2Cov(X_1, X_2)$ from lecture 3 notes.  Assume that the formula holds for some $n$, then

\begin{align*}
& Var\Bigg( \sum_{i=1}^{n+1} X_i\Bigg) \\
&= Var\Bigg( \sum_{i=1}^{n} X_i + X_{n+1} \Bigg) \\
&= Var\Bigg( \sum_{i=1}^{n} X_i \Bigg) + Var( X_{n+1} ) + 2Cov\Bigg(\sum_{i=1}^{n} X_i, X_{n+1}\Bigg)\\
&= \sum_{i=1}^n Var (X_i) + 2 \sum_{1\le i < j \le n}Cov(X_i, X_j) + Var( X_{n+1} )+2 \Bigg[E\Bigg(X_{n+1}\sum_{i=1}^{n} X_i\Bigg)+E\Bigg(X_{n+1}\Bigg)E\Bigg(\sum_{i=1}^{n} X_i\Bigg)\Bigg]\\
&= \sum_{i=1}^{n+1} Var (X_i) + 2 \sum_{1\le i < j \le n}Cov(X_i, X_j) + 2 \Bigg[\sum_{i=1}^{n}E\Bigg(X_{n+1} X_i\Bigg) + E\Bigg(X_{n+1}\Bigg) \sum_{i=1}^{n} E\Bigg(X_i\Bigg)\Bigg]\\
&= \sum_{i=1}^{n+1} Var (X_i) + 2 \sum_{1\le i < j \le n}Cov(X_i, X_j) + 2 \Bigg[\sum_{i=1}^{n}E\Bigg(X_{n+1} X_i\Bigg) + E\Bigg(X_{n+1}\Bigg)  E\Bigg(X_i\Bigg)\Bigg]\\
&= \sum_{i=1}^{n+1} Var (X_i) + 2 \sum_{1\le i < j \le n}Cov(X_i, X_j) + 2 \sum_{i=1}^{n}Cov\Bigg(X_{n+1} X_i\Bigg) \\
&= \sum_{i=1}^{n+1} Var (X_i) + 2 \sum_{1\le i < j \le n+1}Cov(X_i, X_j) \\
\end{align*}

$\square$

\pagebreak

7. Suppose that $X$ and $Y$ are joint normal, i.e. they have the joint PDF:

$$
f(x, y) = \frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1-\rho^2}}\exp(-(2(1-\rho^2))^{-1}(x^2/\sigma_X^2-2 \rho xy/\sigma_X\sigma_Y+y^2/\sigma_Y^2))
$$

(a) Derive the marginal distributions of $X$ and $Y$, and observe that both normal distributions.

The marginal distribution of $X$ is

\begin{align*}
& f_X(x) \\
&= \int_{-\infty}^{\infty} f(x, y) dy \\
&= \int_{-\infty}^{\infty} \frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1-\rho^2}} \exp\Bigg(-\frac{1}{2(1-\rho^2)}\Bigg(\frac{x^2}{\sigma_X^2}-\frac{2\rho xy}{\sigma_X\sigma_Y}+\frac{y^2}{\sigma_Y^2}\Bigg)\Bigg) dy  \\
&= \frac{1}{\sigma_X  \sqrt{2 \pi} } \int_{-\infty}^{\infty} \frac{1}{\sigma_Y\sqrt{1-\rho^2}\sqrt{2 \pi} } \exp\Bigg(-\frac{1}{2(1-\rho^2)}\Bigg(\frac{x^2}{\sigma_X^2}-\frac{2\rho xy}{\sigma_X\sigma_Y}+\frac{y^2}{\sigma_Y^2} + \frac{\rho^2x^2}{\sigma_X^2} - \frac{\rho^2x^2}{\sigma_X^2}\Bigg)\Bigg) dy\\
&= \frac{1}{\sigma_X  \sqrt{2 \pi} } \int_{-\infty}^{\infty} \frac{1}{\sigma_Y\sqrt{1-\rho^2}\sqrt{2 \pi} } \exp\Bigg(-\frac{1}{2(1-\rho^2)}\Bigg(\frac{y^2}{\sigma_Y^2} - \frac{2\rho xy}{\sigma_X\sigma_Y} + \frac{\rho^2x^2}{\sigma_X^2}\Bigg) - \frac{1}{2(1-\rho^2)} \Bigg(\frac{x^2}{\sigma_X^2} - \frac{\rho^2x^2}{\sigma_X^2}\Bigg)\Bigg) dy\\
&= \frac{1}{\sigma_X  \sqrt{2 \pi} } \int_{-\infty}^{\infty} \frac{1}{\sigma_Y\sqrt{1-\rho^2}\sqrt{2 \pi} } \exp\Bigg(-\frac{1}{2(1-\rho^2)} \Bigg(\frac{y^2}{\sigma_Y^2} - \frac{2\rho xy(\sigma_Y/\sigma_X)}{\sigma_Y^2} + \frac{\rho^2x^2(\sigma_Y^2/\sigma_X^2)}{\sigma_Y^2}\Bigg) - \frac{(1-\rho^2)}{2(1-\rho^2)} \Bigg(\frac{x^2}{\sigma_X^2}\Bigg)\Bigg) dy\\
&= \frac{1}{\sigma_X  \sqrt{2 \pi} } \int_{-\infty}^{\infty} \frac{1}{\sigma_Y\sqrt{1-\rho^2}\sqrt{2 \pi} } \exp\Bigg(-\frac{(y-\rho x(\sigma_Y/\sigma_X))^2}{2(1-\rho^2)\sigma_Y^2} - \frac{x^2}{2\sigma_X^2}\Bigg) dy\\
&= \frac{1}{\sigma_X  \sqrt{2 \pi} } \exp\Bigg( - \frac{x^2}{2\sigma_X^2}\Bigg)\int_{-\infty}^{\infty} \frac{1}{\sigma_Y\sqrt{1-\rho^2}\sqrt{2 \pi} } \exp\Bigg(-\frac{(y-\rho x(\sigma_Y/\sigma_X))^2}{2(1-\rho^2)\sigma_Y^2} \Bigg) dy\\
&= \frac{1}{\sigma_X  \sqrt{2 \pi} } \exp\Bigg( - \frac{x^2}{2\sigma_X^2}\Bigg)
\end{align*}

Thus, $X \sim Normal(0, \sigma_X^2)$.  By swapping $x$ and $y$ in the algebra above, we see that $Y \sim Normal(0, \sigma_Y^2)$.

\pagebreak

(b) Derive the conditional distribution of $Y$ given $X=x$. Observe that it is also a normal distribution.

\begin{align*}
f_{Y|X}(y|x) &= \frac{f_{X,Y}(x, y)}{f_X(x)} \\
&= \frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1-\rho^2}} \exp\Bigg(-\frac{1}{2(1-\rho^2)}\Bigg(\frac{x^2}{\sigma_X^2}-\frac{2\rho xy}{\sigma_X\sigma_Y}+\frac{y^2}{\sigma_Y^2}\Bigg)\Bigg) \Bigg[\frac{1}{ \sigma_X\sqrt{2\pi}} \exp \Bigg(-\frac{x^2}{2\sigma_X^2}\Bigg)\Bigg]^{-1} \\
&= \frac{1} { \sigma_Y \sqrt{1- \rho^2} \sqrt{2\pi} } \exp\Bigg(-\frac{1}{2(1-\rho^2)} \Bigg(\frac{x^2}{\sigma_X^2}-\frac{2\rho xy}{\sigma_X\sigma_Y}+\frac{y^2}{\sigma_Y^2}\Bigg)+\frac{x^2}{2\sigma_X^2}\Bigg) \\
&= \frac{1} { \sigma_Y \sqrt{1- \rho^2} \sqrt{2\pi}} \exp\Bigg(-\frac{1}{2(1-\rho^2)} \Bigg(\frac{x^2}{\sigma_X^2}-\frac{2\rho xy}{\sigma_X\sigma_Y}+\frac{y^2}{\sigma_Y^2}-\frac{x^2(1-\rho^2)}{\sigma_X^2}\Bigg)\Bigg) \\
&= \frac{1} { \sigma_Y \sqrt{1- \rho^2} \sqrt{2\pi}} \exp\Bigg(-\frac{1}{2(1-\rho^2)} \Bigg(\frac{y^2}{\sigma_Y^2} - \frac{2\rho xy}{\sigma_X\sigma_Y} + \frac{x^2\rho^2}{\sigma_X^2}\Bigg)\Bigg) \\
&= \frac{1} { \sigma_Y \sqrt{1- \rho^2} \sqrt{2\pi}} \exp\Bigg(-\frac{1}{2(1-\rho^2)} \Bigg(\frac{y}{\sigma_Y}-\frac{x\rho}{\sigma_X}\Bigg)^2\Bigg) \\
&= \frac{1} { \sigma_Y \sqrt{1- \rho^2} \sqrt{2\pi}} \exp\Bigg(-\frac{1}{2} \frac{(y-\frac{\sigma_Y}{\sigma_X}\rho x)^2}{(1-\rho^2)\sigma_Y^2}\Bigg)
\end{align*}

Observe that $Y|X \sim Normal(\frac{\sigma_Y}{\sigma_X}\rho x, \sigma_Y(1-\rho^2))$.

\pagebreak

(c) Derive the joint distribution of $(X, Z)$ where $Z = (Y/\sigma_Y)-(\rho X/\sigma_X)$, and then show that $X$ and $Z$ are independent.

Define $g : \R^2 \to \R^2$ such that $g(x, y) = (x, (y/\sigma_Y)-(\rho x)/\sigma_X)$.  Notice that $g$ is one-to-one, so it is invertible. Define $h = g^{-1}$ such that $h(x, z) = (x, \sigma_Y(z+\rho x /\sigma_X))$. The determinant of the Jacobian of the tranformation is

\begin{align*}
|J| &= \begin{vmatrix} \frac{\partial h_1(x, z)}{\partial x} & \frac{\partial h_1(x, z)}{\partial z} \\ \frac{\partial h_2(x, z)}{\partial x} & \frac{\partial h_2(x, z)}{\partial z} \end{vmatrix}\\
&= \begin{vmatrix} 1 & 0 \\ \frac{\rho\sigma_Y}{\sigma_X} & \sigma_Y \end{vmatrix}\\
&= \sigma_Y
\end{align*}

Thus, from lecture 3 notes, we know that the joint distribution 

\begin{align*}
& f_{X, Z}(x, z) \\
&= f_{X, Y}(h(x, z))|J| \\
&= \frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1-\rho^2}} \exp\Bigg(-\frac{1}{2(1-\rho^2)}\Bigg(\frac{x^2}{\sigma_X^2}-\frac{2\rho x(\sigma_Y(z+\rho x /\sigma_X))}{\sigma_X\sigma_Y}+\frac{(\sigma_Y(z+\rho x /\sigma_X))^2}{\sigma_Y^2}\Bigg)\Bigg) \sigma_Y\\
&= \frac{1}{2 \pi \sigma_X \sqrt{1-\rho^2}} \exp\Bigg(-\frac{1}{2(1-\rho^2)} \Bigg(\frac{x^2}{\sigma_X^2}-\frac{2 \rho x(z+\rho x /\sigma_X)}{\sigma_X}+(z+\rho x /\sigma_X)^2\Bigg)\Bigg) \\
&= \frac{1}{2 \pi \sigma_X \sqrt{1-\rho^2}} \exp\Bigg(-\frac{1}{2(1-\rho^2)} \Bigg(\frac{x^2}{\sigma_X^2} - \frac{2 \rho xz}{\sigma_X} - \frac{2 \rho^2 x^2}{\sigma_X^2} + z^2 + \frac{2z\rho x}{\sigma_X}+\frac{\rho^2 x^2}{\sigma_X^2}\Bigg)\Bigg) \\
&= \frac{1}{2 \pi \sigma_X \sqrt{1-\rho^2}} \exp\Bigg(-\frac{1}{2(1-\rho^2)} \Bigg(\frac{x^2}{\sigma_X^2}  - \frac{ \rho^2 x^2}{\sigma_X^2} + z^2 \Bigg)\Bigg) \\
&= \frac{1}{2 \pi \sigma_X \sqrt{1-\rho^2}} \exp\Bigg(-\frac{1}{2(1-\rho^2)} \Bigg(\frac{(1-\rho^2)x^2}{\sigma_X^2} + z^2 \Bigg)\Bigg) \\
&= \frac{1}{2 \pi \sigma_X \sqrt{1-\rho^2}} \exp\Bigg(-\frac{1}{2} \Bigg(\frac{x^2}{\sigma_X^2} + \frac{z^2}{(1-\rho^2)} \Bigg)\Bigg) \\
&= \frac{1}{\sqrt{2 \pi} \sigma_X } \exp\Bigg(-\frac{1}{2} \Bigg(\frac{x^2}{\sigma_X^2} \Bigg)\Bigg)\frac{1}{\sqrt{2 \pi} \sqrt{1-\rho^2}} \exp\Bigg(-\frac{1}{2} \Bigg( \frac{z^2}{(1-\rho^2)} \Bigg)\Bigg) \\
&= f_X(x) \frac{1}{\sqrt{2 \pi} \sqrt{1-\rho^2}} \exp\Bigg(-\frac{1}{2} \Bigg( \frac{z^2}{(1-\rho^2)} \Bigg)\Bigg) \\
\end{align*}
Thus, $X$ and $Z$ are independent with $Z \sim Normal(0, 1-\rho^2)$.

\pagebreak

8. Consider a function $g : \R \to \R$. Recall that the inverse image of a set $A$, denoted $g^{-1}(A)$ is $g^{-1}(A)=\{x \in \R: g(x) \in A\}$.  Let there be functions $g_1: \R \to \R$ and $g_2: \R \to \R$.  Let $X$ and $Y$ be two random variables that are independent.  Suppose that $g_1$ and $g_2$ are both Borel-measurable, which means that $g_1^{-1}(A)$ and $g_2^{-1}(A)$ are both in the Borel $\sigma$-field whenever $A$ is in the Borel $\sigma$-field.  Show that the two random variables $Z := g_1(X)$ and $W:=g_2(Y)$ are independent.  (Hint: use the 1st or the 2nd definition of independence.)

Proof: Let $A, B$ be events in the Borel $\sigma$-field. Then,

$$
P(Z \in A, W \in B) = P(g_1(X) \in A, g_2(Y) \in B) = P(X \in g_1^{-1}(A), Y \in g_2^{-1}(B))
$$

Since $g_1, g_2$ are Borel measurable, $g_1^{-1}(A), g_2^{-1}(B)$ are events in the Borel $\sigma$-field. Thus, by the independence of $X$ and $Y$:

$$
P(X \in g_1^{-1}(A), Y \in g_2^{-1}(B)) = P(X \in g_1^{-1}(A)) P(Y \in g_2^{-1}(B)) = P(g_1(X) \in A) P(g_2(Y) \in B) = P(Z \in A)P(W \in B)
$$

Thus, $Z$ and $W$ are independent. $\square$
