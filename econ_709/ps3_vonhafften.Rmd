---
title: "ECON 709 - PS 3"
author: "Alex von Hafften^[I worked on this problem set with a study group of Michael Nattinger, Andrew Smith, and Ryan Mather. I also discussed problems with Emily Case, Sarah Bass, and Danny Edgel.]"
date: "9/27/2020"
output: pdf_document
header-includes:
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. A random point $(X, Y)$ is distributed uniformly on the square with vertices $(1, 1), (1, -1), (-1, 1)$, and $(-1, -1)$. That is, the joint PDF is $f(x, y) = 1/4$ on the square and $f(x, y) = 0$ outside the square. Determine the probability of the following events:

(a) $X^2 + Y^2 < 1$

$X^2 + Y^2 < 1 \implies -\sqrt{1 - X^2} < Y < \sqrt{1 - X^2}$

$$ \int_{-1}^1 \int_{-\sqrt{1 - x^2}}^{\sqrt{1 - x^2}} \frac{1}{4}dydx =  \int_{-1}^1 \Bigg[\frac{1}{4}y\Bigg]_{-\sqrt{1 - x^2}}^{\sqrt{1 - x^2}}dx =  \frac{1}{2}\int_{-1}^1 \sqrt{1 - x^2}dx$$

Define $x = \sin \theta \implies dx = \cos \theta d \theta$.

\begin{align*}
\frac{1}{2}\int_{-1}^1 \sqrt{1 - x^2}dx &= \frac{1}{2}\int_{-\pi/2}^{\pi/2} \sqrt{1 - (\sin \theta)^2} \cos \theta d\theta \\
                                        &= \frac{1}{2}\int_{-\pi/2}^{\pi/2} \cos^2 \theta d\theta \\
                                        &= \frac{1}{4}\int_{-\pi/2}^{\pi/2} 1+\cos (2\theta) d\theta \\
                                        &= \frac{1}{4} \Bigg[\theta+\frac{\sin (2\theta)}{2}\Bigg]_{-\pi/2}^{\pi/2} \\
                                        &= \frac{1}{4} \Bigg[(\pi/2)+\frac{0}{2} - (-\pi/2) - \frac{0}{2}\Bigg] \\
                                        &= \frac{\pi}{4}
\end{align*}

(b) $|X + Y| < 2$

$|X + Y| < 2 \implies -2 < X +Y < 2 \implies -2-X<Y<2-X$. Since $X$ ranges from -1 to 1, $-2-X<Y<2-X \implies -1<Y<1$

$$ \int_{-1}^1 \int_{-1}^{1} \frac{1}{4}dydx = \frac{1}{4} \int_{-1}^1 [ y ]_{-1}^{1} dx  = \frac{1}{2} \int_{-1}^1 dx = \frac{1}{2} [x]_{-1}^1 = 1$$

\pagebreak

2. Let the joint PDF of $X$ and $Y$ be given by $f(x, y) = g(x)h(y)$ $\forall x, y \in \R$ for some functions $g(x)$ and $h(y)$. Let $a$ denote $\int_{-\infty}^\infty g(x)dx$ and $b$ denote $\int_{-\infty}^\infty h(x)dx$

(a) What conditions $a$ and $b$ should satisfy in order for $f(x, y)$ to be a bivariate PDF?

For $f(x, y)$ to be a PDF, it should integrate to one:

\begin{align*}
\int_{-\infty}^\infty \int_{-\infty}^\infty f(x, y) dx dy &= 1 \\
\implies \int_{-\infty}^\infty \int_{-\infty}^\infty g(x)h(y) dx dy &= 1 \\
\implies \int_{-\infty}^\infty g(x) dx \int_{-\infty}^\infty h(y) dy &= 1 \\
\implies a b &= 1 \\
\implies a &= b^{-1} \\
\end{align*}

(b) Find the marginal PDF of $X$ and $Y$.

The marginal PDF of $X$:

$$
f_X(x) = \int_{-\infty}^\infty f(x, y) dy = \int_{-\infty}^\infty g(x)h(y) dy = g(x)\int_{-\infty}^\infty h(y) dy = b \cdot g(x)
$$

The marginal PDF of $Y$:

$$
f_Y(y) = \int_{-\infty}^\infty f(x, y) dx = \int_{-\infty}^\infty g(x)h(y) dx = h(y)\int_{-\infty}^\infty g(x) dx = a \cdot h(y)
$$

(c) Show that $X$ and $Y$ are independent.

Proof: $X$ and $Y$ are independent if the product of their marginal distributions is their joint distribution:

\begin{align*}
f_X(x) \cdot f_Y(y) &= b \cdot g(x) \cdot a \cdot h(y) \\
                    &= b \cdot g(x) \cdot b^{-1} \cdot h(y) \\
                    &= g(x) \cdot h(y) \\
                    &= f(x, y) \\
\end{align*}

$\square$

\pagebreak

3. Let the joint PDF of $X$ and $Y$ be given by 

\begin{align*}
f(x, y) = \begin{cases} cxy & \text{ if } x, y \in [0, 1], x+y \le 1 \\ 0, & \text{ otherwise } \end{cases}
\end{align*}

(a) Find the value of $c$ such that $f(x, y)$ is a joint PDF.

\begin{align*}
        \int_0^1 \int_0^{1-x} f(x, y) dy dx &= 1 \\
\implies    \int_0^1 \int_0^{1-x} cxy dy dx &= 1 \\
\implies c\int_0^1 \Big[ \frac{xy^2}{2} \Big]_{y=0}^{1-x} dx &= 1\\
\implies \frac{c}{2} \int_0^1 x(1-x)^2 dx &= 1\\
\implies \frac{c}{2}\Big[ \frac{x^2}{2}-\frac{2x^3}{3}+\frac{x^4}{4} \Big]_{x=0}^1 &= 1\\
\implies \frac{c}{2}\Big[ \frac{1}{2}-\frac{2}{3}+\frac{1}{4} \Big] &= 1\\
\implies c &= 24\\
\end{align*}

(b) Find the marginal distributions of $X$ and $Y$.

$$
f_X(x) = \int_0^{1-x} f(x,y) dy = \int_0^{1-x} 24xy dy = \Big[ 12xy^2 \Big]_{y=0}^{1-x} = \begin{cases} 12x(1-x)^2, x \in [0, 1] \\ 0, \text{ otherwise.} \end{cases}
$$

$$
f_Y(y) = \int_0^{1-y} f(x,y) dx = \int_0^{1-y} 24xy dx = \Big[ 12x^2y \Big]_{x=0}^{1-y} = \begin{cases} 12(1-y)^2 y, y \in [0, 1] \\ 0, \text{ otherwise.} \end{cases}
$$

(c) Are $X$ and $Y$ independent? Compare your answer to Problem 2 and discuss.

$X$ and $Y$ independent if the product of the marginal distributions equals their joint distribution at all points in the support.  If $x=y=0.9$, $f(0.9, 0.9) = 0$ because $(0.9, 0.9)$ is not in the support, $x+y=0.9+0.9=1.8 > 1$. But each marginal distribution is define over $[0,1]$, so the product of the marginals is positive at $(0.9, 0.9)$: $f_X(0.9)f_Y(0.9) = [12(0.9)(1-(0.9))^2][12(1-0.9)^2 (0.9)] = 0.0117$.

In (2), the support for the joint distribution is $\R^2$, whereas the support for the joint distribution depends on the realization of the random variable.

\pagebreak

4. Show that any random variable is uncorrelated with a constant.

Proof: Let $a \in \R$ and $X$ be a random variable with distribution $F_X$. Define random variable $Y$ as the degenerate random variable that equals $a$.  Thus, the distribution $Y$ is

\begin{align*}
F_Y(y) = \begin{cases} 0, & y<a \\ 1, & y \ge a \end{cases}
\end{align*}

To show $X$ is uncorrelated with a constant, I show that $X$ and $Y$ are independent and then, by a theorem in the Lecture 3 Notes, we know that $X$ and $Y$ are uncorrelated.

To find the joint distribution of $X$ and $Y$, consider two cases: $y < a$ and $y \ge a$. For $y < a$,

\begin{align*}
F(x, y) &= P(X \le x \text{ and } Y \le y)\\
        &= P(X \le x \text{ and } Y \le a) \\
        &= 0
\end{align*}

For $y \ge a$:

\begin{align*}
F(x, y) &= P(X \le x \text{ and } Y \le y)\\
        &= P(X \le x) \\
        &= F_X(x)
\end{align*}

Thus, the joint distribution is

\begin{align*}
F(x, y) = \begin{cases} 0, & y<a \\ F_X(x), & y \ge a \end{cases}
\end{align*}

The joint distribution equals the product of the marginals:

\begin{align*}
F(x, y) = \begin{cases} 0*F_X(x), & y<a \\ 1*F_X(x), & y \ge a \end{cases} = \begin{cases} F_Y(y)*F_X(x), & y<a \\ F_Y(y)*F_X(x), & y \ge a \end{cases}.
\end{align*}

$\square$

\pagebreak

5. Let $X$ and $Y$ be independent random variables with means $\mu_X, \mu_Y$ and variances $\sigma_X^2, \sigma_Y^2$.  Find an expression for the correlation of $XY$ and $Y$ in terms of these means and variances.

Note that $Var(X) = E(X^2) - E(X)^2 \implies E(X^2) = Var(X) + E(X)^2$.

\begin{align*}
Corr(XY, Y) 
&= \frac{Cov(XY, Y)}{\sqrt{Var(XY) Var(Y)}} \\
&= \frac{E(XY^2)-E(XY)E(Y)}{\sigma_Y\sqrt{E((XY)^2)-E(XY)^2}} \\
&= \frac{E(X)E(Y^2)-E(X)E(Y)E(Y)}{\sigma_Y\sqrt{E(X^2)E(Y^2)-(E(X)E(Y))^2}} \\
&= \frac{\mu_X(Var(Y) + E(Y)^2)-\mu_X\mu_Y^2}{\sigma_Y\sqrt{(Var(X) + E(X)^2)(Var(Y) + E(Y)^2)-(\mu_X\mu_Y)^2}} \\
&= \frac{\mu_X\sigma_Y^2 + \mu_X\mu_Y^2-\mu_X\mu_Y^2}{\sigma_Y\sqrt{\sigma_Y^2\sigma_X^2 + \mu_X^2\sigma_Y^2 + \mu_Y^2\sigma_X^2 + \mu_Y^2\mu_X^2-\mu_X^2\mu_Y^2}} \\
&= \frac{\mu_X\sigma_Y}{\sqrt{\sigma_Y^2\sigma_X^2 + \mu_X^2\sigma_Y^2 + \mu_Y^2\sigma_X^2}}
\end{align*}

\pagebreak

6. Prove the following: For any random vector $(X_1, X_2, ..., X_n)$,

$$
Var\Bigg( \sum_{i=1}^n X_i\Bigg) = \sum_{i=1}^n Var (X_i) + 2 \sum_{1\le i < j \le n}
Cov(X_i, X_j).
$$

Proof by induction.

\pagebreak

7. Suppose that $X$ and $Y$ are joint normal, i.e. they have the joint PDF:

$$
f(x, y) = \frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1-\rho^2}}\exp(-(2(1-\rho^2))^{-1}(x^2/\sigma_X-2xy/\sigma_X\sigma_Y+y^2/\sigma_Y^2))
$$

(a) Derive the marginal distributions of $X$ and $Y$, and observe that both normal distributions.

Gaussian integrals

(b) Derive the conditional distribution of $Y$ given $X=x$. Observe that it is also a normal distribution.

(c) Derive the joint distribution of $(X, Z)$ where $Z = (Y/\sigma_Y)-(\rho X/\sigma_X)$, and then show that $X$ and $Z$ are independent.

Equation 23 in lecture 3.

\pagebreak

8. Consider a function $g : \R \to \R$. Recall that the inverse image of a set $A$, denoted $g^{-1}(A)$ is $g^{-1}(A)=\{x \in \R: g(x) \in A\}$.  Let there be functions $g_1: \R \to \R$ and $g_2: \R \to \R$.  Let $X$ and $Y$ be two random variables that are independent.  Suppose that $g_1$ and $g_2$ are both Borel-measurable, which means that $g_1^{-1}(A)$ and $g_2^{-1}(A)$ are both in the Borel $\sigma$-field whenever $A$ is in the Borel $\sigma$-field.  Show that the two random variables $Z := g_1(X)$ and $W:=g_2(Y)$ are independent.  (Hint: use the 1st or the 2nd definition of independence.)


