---
title: "ECON 709 - PS 5"
author: "Alex von Hafften^[I worked on this problem set with a study group of Michael Nattinger, Andrew Smith, and Ryan Mather. I also discussed problems with Emily Case, Sarah Bass, and Danny Edgel.]"
date: "10/11/2020"
output: pdf_document
header-includes:
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. For the following sequences, show $a_n \to 0$ as $n \to \infty$:

(a) $a_n = 1/n$

Fix $\varepsilon > 0$.  Choose $\bar{n} > \frac{1}{\varepsilon}$. For all $n \ge \bar{n}$,

$$
|1/n - 0| = |1/n| = \varepsilon.
$$

Thus, $a_n = 1/n \to 0$ as $n \to \infty$.

(b) $a_n = \frac{1}{n} \sin(\frac{n \pi}{2})$

Fix $\varepsilon > 0$. Notice that $|\sin(x)|\le1$ $\forall x$. Choose $\bar{n} > \frac{1}{\varepsilon}$. For all $n \ge \bar{n}$,

$$
\Bigg|\frac{1}{n} \sin\Bigg(\frac{n \pi}{2}\Bigg) - 0\Bigg|=\Bigg|\frac{1}{n} \sin\Bigg(\frac{n \pi}{2}\Bigg) \Bigg| \le |1|\Bigg|\frac{1}{n}\Bigg| = \Bigg|\frac{1}{n}\Bigg|\le \varepsilon
$$

Thus, $a_n = \frac{1}{n} \sin(\frac{n \pi}{2}) \to 0$ as $n \to \infty$.

\pagebreak

2. Consider a random variable $X^n$ with the probability function

\begin{align*}
X_n = \begin{cases} -n, & \text{with probability } 1/n \\ 
                    0,  & \text{with probability } 1-2/n \\ 
                    n,  & \text{with probability } 1/n \end{cases}
\end{align*}

(a) Does $X_n \to_p 0$ as $n\to \infty$?

Fix $\varepsilon > 0$.  Choose $\bar{n} > \varepsilon$.  For $n \ge \bar{n}$,

$$
P(|X_n| \ge \varepsilon) \le P(|X_n| \ge n) = P(X_n = -n) + P(X_n = n) = 1/n + 1/n = 2/n
$$

Since $1/n \to 0$, $2/n \to 0$. Thus, $X_n \to_p 0$ as $n\to \infty$.

(b) Calculate $E(X_n)$.

$$
E(X_n) = \sum_{x \in \text{Supp}(X)} \pi(x)x = (1/n)*(-n)+(1-2/n)(0)+(1/n)(n)=-1+1=0.
$$

(c) Calculate $Var(X_n)$.

$$
Var(X_n) = E(X_n^2)-E(X_n)^2=E(X_n^2) = \sum_{x \in \text{Supp}(X)} \pi(x) x^2 = (1/n)*(-n)^2+(1-2/n)(0)^2+(1/n)(n)^2 = n+n=2n.
$$

(d) Now suppose the distribution is

\begin{align*}
X_n = \begin{cases} 0, & \text{with probability } 1-1/n \\ 
                    n,  & \text{with probability } 1/n  \end{cases}
\end{align*}

|          Calculate $E(X_n)$.

$$
E(X_n)= \sum_{x \in \text{Supp}(X)} \pi(x) x = (1-1/n)(0)+(1/n)(n)=0+1=1
$$

(e) Conclude that $X_n \to_p 0$ is not sufficient for $E(X_n) \to 0$.

Fix $\varepsilon > 0$. Choose $\bar{n} > \varepsilon$. For $n > \bar{n}$

$$
P(|X_n| \ge \varepsilon) \le P(|X_n| \ge n) = P(X_n = n) = 1/n
$$

Since $1/n \to 0$, $X_n \to_p 0$ as $n \to \infty$.  Thus, $X_n \to_p 0$ is not sufficient for $E(X_n) \to 0$.

\pagebreak

3. A weighted sample mean takes the form $\bar{Y}^* = \frac{1}{n} \sum_{i=1}^n w_iY_i$ for some non-negative constants $w_i$ satisfying $\frac{1}{n}\sum_{i=1}^n w_i =1$. Assume that $Y_i : i =1, ..., n$ are i.i.d.

(a) Show that $\bar{Y}^*$ is unbiased for $\mu=E(Y_i)$.

$$
E(\bar{Y}^*) = E\Bigg( \frac{1}{n} \sum_{i=1}^n w_iY_i  \Bigg)= \frac{1}{n} \sum_{i=1}^n w_i E (Y_i)= \frac{1}{n} \sum_{i=1}^n w_i \mu = (1) \mu = \mu
$$

(b) Calculate $Var(\bar{Y}^*)$.

$$
Var(\bar{Y}^*) = Var\Bigg( \frac{1}{n} \sum_{i=1}^n w_iY_i  \Bigg)= \frac{1}{n^2} \sum_{i=1}^n w_i^2 Var (Y_i)
$$

(c) Show that a sufficient condition for $\bar{Y}^* \to_p \mu$ is that $\frac{1}{n^2} \sum_{i=1}^n w_i^2 \to 0$. (Hint: use the Markov's or Chebyshev's Inequality).

Fix $\varepsilon > 0$. Because $\frac{1}{n^2} \sum_{i=1}^n w_i^2 \to 0$, there exists $\bar{n}$ such that for $n\ge \bar{n}$, 

$$\Bigg| \frac{1}{n^2}\sum_{i=1}^n w_i^2 \Bigg| \le \varepsilon$$

From (a) we know that $E(\bar{Y}^*) = \mu$ and from (b) we know that $Var(\bar{Y}^*) = \frac{1}{n^2} \sum_{i=1}^n w_i^2 Var (Y_i)$, so by Chebychev's Inequality,

$$P(|\bar{Y}^* - \mu| \ge \lambda) \le \frac{Var(\bar{Y}^*)}{\lambda^2} = \frac{\frac{1}{n^2} \sum_{i=1}^n w_i^2 Var (Y_i)}{\lambda^2} \le \frac{\varepsilon Var (Y_i)}{\lambda^2}=\frac{\varepsilon Var (Y_i)}{\Bigg(\sqrt{ Var(Y_i)}\Bigg)^2} = \varepsilon$$

where $\lambda = \sqrt{ Var(Y_i)}$.  Thus $\bar{Y}^* \to_p \mu$.

(d) Show that the sufficient condition for the condition in part (c) is $\max_{i \le n} w_i/n \to 0$.

Fix $\varepsilon > 0$.  Let $\delta = \sqrt{\frac{\varepsilon}{n}}$. Because $\max_{i \le n} w_i/n \to 0$, there exists a $\bar{n}$ such that for $n\ge \bar{n}$, 

\begin{align*}
\Bigg|\max_{i \le n} w_i/n\Bigg| \le \delta 
&\implies |w_i/n| \le \delta \; \forall i \in \{1, ..., n\} \\
&\implies (w_i/n)^2 \le \delta^2 \; \forall i \in \{1, ..., n\} \\
&\implies  \sum_{i=1}^n \frac{w_i^2}{n^2} \le n \delta^2  \\
&\implies  \sum_{i=1}^n \frac{w_i^2}{n^2} \le n \Bigg(\sqrt{\frac{\varepsilon}{n}}\Bigg)^2  \\
&\implies \sum_{i=1}^n \frac{w_i^2}{n^2} \le \varepsilon
\end{align*}

Thus, $\frac{1}{n^2} \sum_{i=1}^n w_i^2 \to 0$.

\pagebreak

4. Take a random sample $\{X_1, ..., X_n\}$. Which statistic converges in probability by the weak law of large numbers and continuous mapping theorem, assuming the moment exists?

(a) $\frac{1}{n} \sum_{i=1}^n X_i^2$

Transform $\{X_1, ..., X_n\}$ to $\{Y_1, ..., Y_n\}$ such that $Y_i = X_i^2$.  Thus, $\{Y_1, ..., Y_n\}$ is an i.i.d. sequence with $E(|Y_i|) = E(X_i^2) = \mu_2 < \infty$.  By the weak law of large numbers $\bar{Y}_N \to_p \mu_2$ as $n \to \infty$. Thus, $\frac{1}{n} \sum_{i=1}^n X_i^2 \to_p \mu_2$ as $n \to \infty$.

(b) $\frac{1}{n} \sum_{i=1}^n X_i^3$

Transform $\{X_1, ..., X_n\}$ to $\{Y_1, ..., Y_n\}$ such that $Y_i = X_i^3$.  Thus, $\{Y_1, ..., Y_n\}$ is an i.i.d. sequence with $E(|Y_i|) = E(X_i^3) = \mu_3 < \infty$.  By the weak law of large numbers $\bar{Y}_N \to_p \mu_3$ as $n \to \infty$. Thus, $\frac{1}{n} \sum_{i=1}^n X_i^3 \to_p \mu_3$ as $n \to \infty$.

(c) $\max_{i \le n}X_i$

This statistic does not converge in probability by the weak law of large numbers and continuous mapping theorem.  Instead we could apply the Fisher-Tippett-Gnedenko theorem, which can characterize the asymptotic distribution of extreme order statistics.

(d) $\frac{1}{n} \sum_{i=1}^n X_i^2 - (\frac{1}{n} \sum_{i=1}^n X_i)^2$

From (a), we know that $\frac{1}{n} \sum_{i=1}^n X_i^2 \to_p \mu_2$.  An immediate result of the weak law of large numbers is $\frac{1}{n} \sum_{i=1}^n X_i \to_p \mu$. By the continuous mapping theorem, $(\frac{1}{n} \sum_{i=1}^n X_i)^2 \to_p \mu^2$ . Thus, $\frac{1}{n} \sum_{i=1}^n X_i^2 - (\frac{1}{n} \sum_{i=1}^n X_i)^2 \to_p \mu_2 - \mu^2$.

(e) $\frac{\sum_{i=1}^n X_i^2}{\sum_{i=1}^n X_i}$ assuming $\mu = E(X_i) > 0$.

From (a), we know that $\frac{1}{n} \sum_{i=1}^n X_i^2 \to_p \mu_2$.  An immediate result of the weak law of large numbers is $\frac{1}{n} \sum_{i=1}^n X_i \to_p \mu$.  Thus, by the Continuous Mapping Theorem, $\frac{n^{-1}\sum_{i=1}^n X_i^2}{n^{-1}\sum_{i=1}^n X_i} = \frac{\sum_{i=1}^n X_i^2}{\sum_{i=1}^n X_i} \to_p \frac{\mu_2}{\mu}$.

(f) $1(\frac{1}{n} \sum_{i=1}^n X_i > 0)$ where

\begin{align*}
1(a) = \begin{cases} 
       1 & \text{if $a$ is true} \\ 
       0 & \text{if $a$ is not true} 
       \end{cases}
\end{align*}

|         is called the indicator function of event $a$.

Notice that $1(\frac{1}{n} \sum_{i=1}^n X_i > 0) \sim Bernoulli(P(\frac{1}{n} \sum_{i=1}^n X_i > 0))$.  By the weak law of large numbers, $\frac{1}{n} \sum_{i=1}^n X_i = \bar{X}_n \to_p \mu$.  So, if $\mu > 0$, $1(\frac{1}{n} \sum_{i=1}^n X_i > 0) \to_p 1$. if $\mu \le 0$, $1(\frac{1}{n} \sum_{i=1}^n X_i > 0) \to_p 0$.

\pagebreak

5. Take a random sample $\{X_1, ..., X_n\}$ where the support $X_i$ is a subset of $(0, \infty)$.  Consider the sample geometric mean $\hat{\mu} = (\Pi_{i=1}^n X_i)^{1/n}$ and population geometric mean $\mu = \exp(E(\log(X)))$. Assuming that $\mu$ is finite, show that $\hat{\mu} \to_p \mu$ as $n \to \infty$.

Assuming that $\mu$ is finite,

$$ 
\log(\hat{\mu}) = \log((\Pi_{i=1}^n X_i)^{1/n}) = \frac{1}{n}\log(\Pi_{i=1}^n X_i) = \frac{1}{n}\sum_{i=1}^n\log( X_i)
$$

By the weak law of large numbers, $\log(\hat{\mu}) = \frac{1}{n}\sum_{i=1}^n\log( X_i) \to_p E(\log(X))$. By the continuous mapping theorem with $g(x)=\exp(x)$, we know that $\hat{\mu} \to_p \exp(E(\log(X))) = \mu$.

6. Let $\mu_k = E(X^k)$ for some integer $k \ge 1$. ($X_i : i = 1, ..., n$ is a i.i.d. sample.)

(a) Write down the natural moment estimator $\hat{\mu}_k$ of $\mu_k$.

(b)

7. ($X_i : i = 1, ..., n$ is a i.i.d. sample.)

8. ($X_i : i = 1, ..., n$ is a i.i.d. sample.)
