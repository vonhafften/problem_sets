---
title: "ECON 709 - PS 5"
author: "Alex von Hafften^[I worked on this problem set with a study group of Michael Nattinger, Andrew Smith, and Ryan Mather. I also discussed problems with Emily Case, Sarah Bass, and Danny Edgel.]"
date: "10/11/2020"
output: pdf_document
header-includes:
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\Z}{\mathbb{Z}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Q}{\mathbb{Q}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. For the following sequences, show $a_n \to 0$ as $n \to \infty$:

(a) $a_n = 1/n$

Fix $\varepsilon > 0$.  Choose $\bar{n} > \frac{1}{\varepsilon}$. For all $n \ge \bar{n}$,

$$
|1/n - 0| = |1/n| = \varepsilon.
$$

Thus, $a_n = 1/n \to 0$ as $n \to \infty$.

(b) $a_n = \frac{1}{n} \sin(\frac{n \pi}{2})$

Fix $\varepsilon > 0$. Notice that $|\sin(x)|\le1$ $\forall x$. Choose $\bar{n} > \frac{1}{\varepsilon}$. For all $n \ge \bar{n}$,

$$
\Bigg|\frac{1}{n} \sin\Bigg(\frac{n \pi}{2}\Bigg) - 0\Bigg|=\Bigg|\frac{1}{n} \sin\Bigg(\frac{n \pi}{2}\Bigg) \Bigg| \le |1|\Bigg|\frac{1}{n}\Bigg| = \Bigg|\frac{1}{n}\Bigg|\le \varepsilon
$$

Thus, $a_n = \frac{1}{n} \sin(\frac{n \pi}{2}) \to 0$ as $n \to \infty$.

\pagebreak

2. Consider a random variable $X^n$ with the probability function

\begin{align*}
X_n = \begin{cases} -n, & \text{with probability } 1/n \\ 
                    0,  & \text{with probability } 1-2/n \\ 
                    n,  & \text{with probability } 1/n \end{cases}
\end{align*}

(a) Does $X_n \to_p 0$ as $n\to \infty$?

Fix $\varepsilon > 0$.  Choose $\bar{n} > \varepsilon$.  For $n \ge \bar{n}$,

$$
P(|X_n| \ge \varepsilon) \le P(|X_n| \ge n) = P(X_n = -n) + P(X_n = n) = 1/n + 1/n = 2/n
$$

Since $1/n \to 0$, $2/n \to 0$. Thus, $X_n \to_p 0$ as $n\to \infty$.

(b) Calculate $E(X_n)$.

$$
E(X_n) = \sum_{x \in \text{Supp}(X)} \pi(x)x = (1/n)*(-n)+(1-2/n)(0)+(1/n)(n)=-1+1=0.
$$

(c) Calculate $Var(X_n)$.

$$
Var(X_n) = E(X_n^2)-E(X_n)^2=E(X_n^2) = \sum_{x \in \text{Supp}(X)} \pi(x) x^2 = (1/n)*(-n)^2+(1-2/n)(0)^2+(1/n)(n)^2 = n+n=2n.
$$

(d) Now suppose the distribution is

\begin{align*}
X_n = \begin{cases} 0, & \text{with probability } 1-1/n \\ 
                    n,  & \text{with probability } 1/n  \end{cases}
\end{align*}

|          Calculate $E(X_n)$.

$$
E(X_n)= \sum_{x \in \text{Supp}(X)} \pi(x) x = (1-1/n)(0)+(1/n)(n)=0+1=1
$$

(e) Conclude that $X_n \to_p 0$ is not sufficient for $E(X_n) \to 0$.

Fix $\varepsilon > 0$. Choose $\bar{n} > \varepsilon$. For $n > \bar{n}$

$$
P(|X_n| \ge \varepsilon) \le P(|X_n| \ge n) = P(X_n = n) = 1/n
$$

Since $1/n \to 0$, $X_n \to_p 0$ as $n \to \infty$.  Thus, $X_n \to_p 0$ is not sufficient for $E(X_n) \to 0$.

\pagebreak

3. A weighted sample mean takes the form $\bar{Y}^* = \frac{1}{n} \sum_{i=1}^n w_iY_i$ for some non-negative constants $w_i$ satisfying $\frac{1}{n}\sum_{i=1}^n w_i =1$. Assume that $Y_i : i =1, ..., n$ are i.i.d.

(a) Show that $\bar{Y}^*$ is unbiased for $\mu=E(Y_i)$.

$$
E(\bar{Y}^*) = E\Bigg( \frac{1}{n} \sum_{i=1}^n w_iY_i  \Bigg)= \frac{1}{n} \sum_{i=1}^n w_i E (Y_i)= \frac{1}{n} \sum_{i=1}^n w_i \mu = (1) \mu = \mu
$$

(b) Calculate $Var(\bar{Y}^*)$.

$$
Var(\bar{Y}^*) = Var\Bigg( \frac{1}{n} \sum_{i=1}^n w_iY_i  \Bigg)= \frac{1}{n^2} \sum_{i=1}^n w_i^2 Var (Y_i)
$$

(c) Show that a sufficient condition for $\bar{Y}^* \to_p \mu$ is that $\frac{1}{n^2} \sum_{i=1}^n w_i^2 \to 0$. (Hint: use the Markov's or Chebyshev's Inequality).

Fix $\varepsilon > 0$. Because $\frac{1}{n^2} \sum_{i=1}^n w_i^2 \to 0$, there exists $\bar{n}$ such that for $n\ge \bar{n}$, 

$$\Bigg| \frac{1}{n^2}\sum_{i=1}^n w_i^2 \Bigg| \le \varepsilon$$

From (a) we know that $E(\bar{Y}^*) = \mu$ and from (b) we know that $Var(\bar{Y}^*) = \frac{1}{n^2} \sum_{i=1}^n w_i^2 Var (Y_i)$, so by Chebychev's Inequality,

$$P(|\bar{Y}^* - \mu| \ge \lambda) \le \frac{Var(\bar{Y}^*)}{\lambda^2} = \frac{\frac{1}{n^2} \sum_{i=1}^n w_i^2 Var (Y_i)}{\lambda^2} \le \frac{\varepsilon Var (Y_i)}{\lambda^2}=\frac{\varepsilon Var (Y_i)}{\Bigg(\sqrt{ Var(Y_i)}\Bigg)^2} = \varepsilon$$

where $\lambda = \sqrt{ Var(Y_i)}$.  Thus $\bar{Y}^* \to_p \mu$.

(d) Show that the sufficient condition for the condition in part (c) is $\max_{i \le n} w_i/n \to 0$.

Fix $\varepsilon > 0$.  Let $\delta = \sqrt{\frac{\varepsilon}{n}}$. Because $\max_{i \le n} w_i/n \to 0$, there exists a $\bar{n}$ such that for $n\ge \bar{n}$, 

\begin{align*}
\Bigg|\max_{i \le n} w_i/n\Bigg| \le \delta 
&\implies |w_i/n| \le \delta \; \forall i \in \{1, ..., n\} \\
&\implies (w_i/n)^2 \le \delta^2 \; \forall i \in \{1, ..., n\} \\
&\implies  \sum_{i=1}^n \frac{w_i^2}{n^2} \le n \delta^2  \\
&\implies  \sum_{i=1}^n \frac{w_i^2}{n^2} \le n \Bigg(\sqrt{\frac{\varepsilon}{n}}\Bigg)^2  \\
&\implies \sum_{i=1}^n \frac{w_i^2}{n^2} \le \varepsilon
\end{align*}

Thus, $\frac{1}{n^2} \sum_{i=1}^n w_i^2 \to 0$.

\pagebreak

4. Take a random sample $\{X_1, ..., X_n\}$. Which statistic converges in probability by the weak law of large numbers and continuous mapping theorem, assuming the moment exists?

(a) $\frac{1}{n} \sum_{i=1}^n X_i^2$

Transform $\{X_1, ..., X_n\}$ to $\{Y_1, ..., Y_n\}$ such that $Y_i = X_i^2$.  Thus, $\{Y_1, ..., Y_n\}$ is an i.i.d. sequence with $E(|Y_i|) = E(X_i^2) = \mu_2 < \infty$.  By the weak law of large numbers $\bar{Y}_N \to_p \mu_2$ as $n \to \infty$. Thus, $\frac{1}{n} \sum_{i=1}^n X_i^2 \to_p \mu_2$ as $n \to \infty$.

(b) $\frac{1}{n} \sum_{i=1}^n X_i^3$

Transform $\{X_1, ..., X_n\}$ to $\{Y_1, ..., Y_n\}$ such that $Y_i = X_i^3$.  Thus, $\{Y_1, ..., Y_n\}$ is an i.i.d. sequence with $E(|Y_i|) = E(X_i^3) = \mu_3 < \infty$.  By the weak law of large numbers $\bar{Y}_N \to_p \mu_3$ as $n \to \infty$. Thus, $\frac{1}{n} \sum_{i=1}^n X_i^3 \to_p \mu_3$ as $n \to \infty$.

(c) $\max_{i \le n}X_i$

This statistic does not converge in probability by the weak law of large numbers and continuous mapping theorem.  Instead we could apply the Fisher-Tippett-Gnedenko theorem, which can characterize the asymptotic distribution of extreme order statistics.

(d) $\frac{1}{n} \sum_{i=1}^n X_i^2 - (\frac{1}{n} \sum_{i=1}^n X_i)^2$

From (a), we know that $\frac{1}{n} \sum_{i=1}^n X_i^2 \to_p \mu_2$.  An immediate result of the weak law of large numbers is $\frac{1}{n} \sum_{i=1}^n X_i \to_p \mu$. By the continuous mapping theorem, $(\frac{1}{n} \sum_{i=1}^n X_i)^2 \to_p \mu^2$ . Thus, $\frac{1}{n} \sum_{i=1}^n X_i^2 - (\frac{1}{n} \sum_{i=1}^n X_i)^2 \to_p \mu_2 - \mu^2$.

(e) $\frac{\sum_{i=1}^n X_i^2}{\sum_{i=1}^n X_i}$ assuming $\mu = E(X_i) > 0$.

From (a), we know that $\frac{1}{n} \sum_{i=1}^n X_i^2 \to_p \mu_2$.  An immediate result of the weak law of large numbers is $\frac{1}{n} \sum_{i=1}^n X_i \to_p \mu$.  Thus, by the Continuous Mapping Theorem, $\frac{n^{-1}\sum_{i=1}^n X_i^2}{n^{-1}\sum_{i=1}^n X_i} = \frac{\sum_{i=1}^n X_i^2}{\sum_{i=1}^n X_i} \to_p \frac{\mu_2}{\mu}$.

(f) $1(\frac{1}{n} \sum_{i=1}^n X_i > 0)$ where

\begin{align*}
1(a) = \begin{cases} 
       1 & \text{if $a$ is true} \\ 
       0 & \text{if $a$ is not true} 
       \end{cases}
\end{align*}

|         is called the indicator function of event $a$.

Notice that $1(\frac{1}{n} \sum_{i=1}^n X_i > 0) \sim Bernoulli(P(\frac{1}{n} \sum_{i=1}^n X_i > 0))$.  By the weak law of large numbers, $\frac{1}{n} \sum_{i=1}^n X_i = \bar{X}_n \to_p \mu$.  So, if $\mu > 0$, $1(\frac{1}{n} \sum_{i=1}^n X_i > 0) \to_p 1$. if $\mu \le 0$, $1(\frac{1}{n} \sum_{i=1}^n X_i > 0) \to_p 0$.

\pagebreak

5. Take a random sample $\{X_1, ..., X_n\}$ where the support $X_i$ is a subset of $(0, \infty)$.  Consider the sample geometric mean $\hat{\mu} = (\Pi_{i=1}^n X_i)^{1/n}$ and population geometric mean $\mu = \exp(E(\log(X)))$. Assuming that $\mu$ is finite, show that $\hat{\mu} \to_p \mu$ as $n \to \infty$.

Assuming that $\mu$ is finite,

$$ 
\log(\hat{\mu}) = \log((\Pi_{i=1}^n X_i)^{1/n}) = \frac{1}{n}\log(\Pi_{i=1}^n X_i) = \frac{1}{n}\sum_{i=1}^n\log( X_i)
$$

By the weak law of large numbers, $\log(\hat{\mu}) = \frac{1}{n}\sum_{i=1}^n\log( X_i) \to_p E(\log(X))$. By the continuous mapping theorem with $g(x)=\exp(x)$, we know that $\hat{\mu} \to_p \exp(E(\log(X))) = \mu$.

6. Let $\mu_k = E(X^k)$ for some integer $k \ge 1$. 

(a) Write down the natural moment estimator $\hat{\mu}_k$ of $\mu_k$.

For i.i.d. sample $X_i : i = 1, ..., n$, the "plug-in" estimator is the sample moment:

$$
\hat{\mu}_k = \frac{1}{n} \sum_{i=1}^n X_i^k
$$

(b) Find the asymptotic distribution of $\sqrt{n}(\hat{\mu}_k-\mu_k)$ as $n \to \infty$, assuming that $E(X^{2k}) < \infty$.

Notice that the $E(X_i^k) = \mu_k$ and $Var(X_i^k) = E(X_i^{2k})-(\mu_k)^2 = \mu_{2k}-\mu_k^2$.  Thus, by the central limit theorem, $\sqrt{n}(\hat{\mu}_k-\mu_k)\to_d N(0, \mu_{2k}-\mu^2_k)$.

7. Let $m_k = (E(X_k))^{1/k}$ for some integer $k \ge 1$.

(a) Write down the natural moment estimator $\hat{m}_k$ of $m_k$.

For i.i.d. sample $X_i : i = 1, ..., n$, the "plug-in" estimator is:

$$
\hat{m}_k = \Bigg( \frac{1}{n}\sum_{i=1}^n X_i^k \Bigg)^{1/k}
$$

(b) Find the asymptotic distribution of $\sqrt{n}(\hat{m}_k-m_k)$ as $n \to \infty$, assuming that $E(X^{2k}) < \infty$.

From 6(b), we know that $\sqrt{n}(\hat{\mu}_k-\mu_k)\to_d N(0, \mu_{2k}-\mu^2_k)$. Define $g(x)=x^{1/k}$ for some $k \in \N$.  Notice that $g$ is continuous for all values of $k$.  Furthermore, $g(\hat{\mu}_k) = \hat{m}_k$, $g(\mu_k) = m_k$, and $g'(\mu_k) = (1/k)\mu_k^{(1-k)/k}$. Thus, by the Delta Method,  

\begin{align*}
\sqrt{n}(g(\hat{\mu}_k)-g(\mu_k)) &\to_d N(0, (g'(\mu_k))^2(\mu_{2k}-\mu^2_k)) \\
\sqrt{n}(\hat{m}_k-m_k) &\to_d N(0, ((1/k)\mu_k^{(1-k)/k})^2(\mu_{2k}-\mu^2_k)) \\
\sqrt{n}(\hat{m}_k-m_k) &\to_d N\Bigg(0, \frac{\mu_k^{2/k}   (\mu_{2k}-\mu^2_k)}{\mu_k^{2}k^2}\Bigg) \\
\end{align*}

\pagebreak

8. Suppose $\sqrt{n}(\hat{\mu}-\mu) \to_d N(0, v^2)$ and set $\beta = \mu^2$ and $\hat{\beta} = \hat{\mu}^2$.

(a) Use the Delta Method to obtain an asymptotic distribution for $\sqrt{n}(\hat{\beta}-\beta)$.

Define $g(x) = x^2$.  Notice that $g$ is continuous, $\beta = g(\mu)$, and $\hat{\beta} = g(\hat{\mu})$. Thus, by the Delta Method,

\begin{align*}
\sqrt{n}(\hat{\beta}-\beta) &\to_d N(0, (2\mu)^2v^2) \\
\implies \sqrt{n}(\hat{\beta}-\beta) &\to_d N(0, 4 \beta v^2)
\end{align*}

(b) Now suppose $\mu = 0$. Describe what happens to the asymptotic distribution from the previous part.

If $\mu = 0$, then $\beta=(0)^2=0$ and the variance of the asymptotic distribution is zero, so it become a degenerate normal distribution at zero:

$$
\sqrt{n}(\hat{\beta}-\beta) = 0
\implies \hat{\beta} = \beta = 0
$$

(c) Improve on the previous answer. Under the assumption $\mu = 0$, find the asymptotic distribution of $n\hat{\beta}$.

\begin{align*}
\sqrt{n}\hat{\mu} \to_d N(0, v^2) 
&\implies \frac{\sqrt{n}\hat{\mu}}{v} \to_d N(0,1) \\
&\implies \frac{n\hat{\mu}^2}{v^2} \to_d \chi^2_1 \\
&\implies n\hat{\beta} \to_d \Gamma(1/2, 2v^2)
\end{align*}

$n\hat{\beta}$ is distributed gamma with shape parameter, $\alpha = 1/2$, and scale parameter, $\beta = 2v^2$.

(d) Comment on the differences between the answers in parts (a) and (c).

The main difference between (a) and (c) is the term, $n^\alpha$.  In (a), $\alpha = 1/2$ so $n^\alpha$ increases slower than the decrease in the variance of $\hat{\beta}$. Thus, as $n \to \infty$, the asymptotic distribution of $\sqrt{n}(\hat{\beta}- \beta)$ is a degenerate distribution with unit mass at zero.  In (c), $\alpha = 1$ so $n^\alpha$ increases at the same rate as the variance of $\hat{\beta}$ decreases. Thus, as $n \to \infty$, the asymptotic distribution of $n\hat{\beta}$ is non-degenerate.